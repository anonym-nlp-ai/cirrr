{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement V8: FiQA + InsuranceQA + MedQA + MedCQA\n",
    "Reviewed Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# Core scientific and ML libraries\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Key libraries for this implementation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import simpledorff # For Krippendorff's Alpha\n",
    "\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./../../../../\")\n",
    "from aimw.app.services.factory import runnable_system\n",
    "from aimw.app.core.ai_config import get_ai_settings\n",
    "from aimw.app.utils import common_utils\n",
    "from aimw.app.schemas.enum.ai_enums import Role\n",
    "from loguru import logger\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./output/predictions/classifier_cross_model_prediction_random_400_ALL_M5_20250706_091512.json\", encoding=\"utf-8\") as f:\n",
    "    document_list = json.load(f)\n",
    "print(f\"Total: {len(document_list)}\")\n",
    "\n",
    "\n",
    "# --- Use this function to visualize ---\n",
    "def plot_dendrogram(subtopics):\n",
    "    \"\"\"\n",
    "    Generates and plots a dendrogram for a list of subtopics to visualize their hierarchy.\n",
    "    \"\"\"\n",
    "    if len(subtopics) < 2:\n",
    "        print(\"Need at least 2 subtopics to create a dendrogram.\")\n",
    "        return\n",
    "\n",
    "    print(\"Encoding subtopics for dendrogram...\")\n",
    "    embeddings = sentence_model.encode(subtopics)\n",
    "\n",
    "    # The 'ward' linkage method is often better for visualization and creating balanced clusters.\n",
    "    print(\"Creating linkage matrix...\")\n",
    "    linked = linkage(embeddings, method='ward', metric='euclidean')\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    print(\"Plotting dendrogram...\")\n",
    "    dendrogram(\n",
    "        linked,\n",
    "        orientation='top',\n",
    "        labels=subtopics,\n",
    "        distance_sort='descending',\n",
    "        show_leaf_counts=True,\n",
    "        leaf_rotation=90,\n",
    "        leaf_font_size=8\n",
    "    )\n",
    "    plt.title('Hierarchical Clustering Dendrogram of Subtopics', fontsize=16)\n",
    "    plt.xlabel('Subtopics', fontsize=12)\n",
    "    plt.ylabel('Euclidean Distance', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"subtopic_dendrogram.png\", dpi=300)\n",
    "    plt.show()\n",
    "    print(\"Dendrogram saved as 'subtopic_dendrogram.png'\")\n",
    "\n",
    "# --- 1. SETUP: LOAD MODEL AND DATA ---\n",
    "\n",
    "# Load a powerful sentence embedding model\n",
    "# This model is adept at capturing nuanced semantic meaning in short phrases\n",
    "print(\"Loading sentence embedding model...\")\n",
    "sentence_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Your provided data\n",
    "# In a real scenario, this would be loaded from a file (e.g., json.load)\n",
    "data = document_list #[:50]\n",
    "\n",
    "\n",
    "# --- 2. PAIRWISE AGREEMENT FUNCTIONS (FROM ORIGINAL SCRIPT) ---\n",
    "\n",
    "def get_subtopic_embeddings(subtopics):\n",
    "    \"\"\"Encodes a list of subtopics into sentence embeddings.\"\"\"\n",
    "    if not subtopics:\n",
    "        return np.array([])\n",
    "    return sentence_model.encode(subtopics)\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    \"\"\"Calculates Jaccard similarity between two lists of strings.\"\"\"\n",
    "    if not list1 and not list2: return 1.0\n",
    "    if not list1 or not list2: return 0.0\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0.0\n",
    "\n",
    "def calculate_semantic_agreement(subtopics1, subtopics2, similarity_threshold=0.7, method=\"soft-f1\"):\n",
    "    \"\"\"Calculates a semantic agreement score between two lists of subtopics.\"\"\"\n",
    "    if method == \"jaccard\":\n",
    "        return jaccard_similarity(subtopics1, subtopics2)\n",
    "\n",
    "    if not subtopics1 or not subtopics2:\n",
    "        return 0.0\n",
    "\n",
    "    embeddings1 = get_subtopic_embeddings(subtopics1)\n",
    "    embeddings2 = get_subtopic_embeddings(subtopics2)\n",
    "    similarity_matrix = cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "    if method == \"soft-f1\":\n",
    "        # Precision-like score\n",
    "        matches1_to_2 = np.sum(np.max(similarity_matrix, axis=1) >= similarity_threshold)\n",
    "        precision_like = matches1_to_2 / len(subtopics1)\n",
    "        # Recall-like score\n",
    "        matches2_to_1 = np.sum(np.max(similarity_matrix, axis=0) >= similarity_threshold)\n",
    "        recall_like = matches2_to_1 / len(subtopics2)\n",
    "        # F1-like score\n",
    "        if precision_like + recall_like == 0: return 0.0\n",
    "        return 2 * (precision_like * recall_like) / (precision_like + recall_like)\n",
    "\n",
    "    elif method == \"bipartite\":\n",
    "        # Use Hungarian algorithm to find optimal pairings\n",
    "        cost_matrix = 1 - similarity_matrix\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        # Count valid matches above the threshold\n",
    "        valid_matches = np.sum(similarity_matrix[row_ind, col_ind] >= similarity_threshold)\n",
    "        # Calculate F1 score\n",
    "        precision = valid_matches / len(subtopics1) if len(subtopics1) > 0 else 0\n",
    "        recall = valid_matches / len(subtopics2) if len(subtopics2) > 0 else 0\n",
    "        if precision + recall == 0: return 0.0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "    elif method == \"avg-cosine\":\n",
    "        return np.mean(similarity_matrix)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method specified.\")\n",
    "\n",
    "# --- 3. KRIPPENDORFF'S ALPHA IMPLEMENTATION ---\n",
    "\n",
    "def get_conceptual_topics_for_doc(doc_data, distance_threshold=0.7, n_clusters=None):\n",
    "    \"\"\"\n",
    "    Performs semantic clustering on all subtopics for a single document\n",
    "    to establish a set of common \"conceptual topics\" for Krippendorff's Alpha.\n",
    "    \n",
    "    Args:\n",
    "        doc_data (dict): The data for a single document.\n",
    "        distance_threshold (float): The linkage distance threshold for clustering. \n",
    "                                   A lower value creates more, finer-grained clusters.\n",
    "                                   1 - cosine similarity.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries for creating the Alpha DataFrame.\n",
    "    \"\"\"\n",
    "    all_models_data = []\n",
    "    docid = doc_data['docid']\n",
    "\n",
    "    # Add our classifier's data\n",
    "    cir3_subtopics = doc_data[\"perpectives\"][\"cir3_classifier_subtopics\"]\n",
    "    if cir3_subtopics:\n",
    "        all_models_data.append({\"model_name\": \"cir3_classifier\", \"subtopics\": cir3_subtopics})\n",
    "\n",
    "    # Add cross-model data\n",
    "    for cross_model in doc_data[\"perpectives\"][\"cross_model_classification\"]:\n",
    "        if cross_model[\"subtopics\"]:\n",
    "            all_models_data.append(cross_model)\n",
    "\n",
    "    if not all_models_data:\n",
    "        return []\n",
    "\n",
    "    # Step 1: Consolidate all subtopics from all models for the document\n",
    "    corpus = [subtopic for model_data in all_models_data for subtopic in model_data['subtopics']]\n",
    "    if not corpus:\n",
    "        return []\n",
    "\n",
    "    corpus_embeddings = sentence_model.encode(corpus)\n",
    "\n",
    "    # Step 2: Cluster the embeddings using Agglomerative Clustering\n",
    "    # This method is good because we don't need to specify the number of clusters beforehand.\n",
    "    # We use cosine distance and a distance threshold to form clusters.\n",
    "    # 'euclidean',     # 'ward' linkage requires 'euclidean' distance\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        #  Exactly one of n_clusters and distance_threshold has to be set, and the other needs to be None.\n",
    "        distance_threshold=distance_threshold if n_clusters is None else None,\n",
    "        metric= 'cosine', # 'euclidean', # 'cosine',\n",
    "        linkage='average' # 'ward' # 'average'\n",
    "    ).fit(corpus_embeddings)\n",
    "    \n",
    "    cluster_labels = clustering.labels_\n",
    "\n",
    "    # Step 3: Assign a canonical label to each cluster\n",
    "    # We'll use the most frequent subtopic in the cluster as its canonical name.\n",
    "    cluster_map = {}\n",
    "    for cluster_id in set(cluster_labels):\n",
    "        indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "        cluster_subtopics = [corpus[i] for i in indices]\n",
    "        # Find the most frequent subtopic to name the cluster\n",
    "        canonical_label = max(set(cluster_subtopics), key=cluster_subtopics.count)\n",
    "        cluster_map[cluster_id] = canonical_label\n",
    "\n",
    "    # Step 4: Create the \"rating matrix\" rows for this document\n",
    "    krippendorff_data = []\n",
    "    corpus_idx = 0\n",
    "    for model_data in all_models_data:\n",
    "        model_name = model_data['model_name']\n",
    "        num_subtopics = len(model_data['subtopics'])\n",
    "        \n",
    "        # Get the cluster assignments for this model's subtopics\n",
    "        model_cluster_ids = cluster_labels[corpus_idx : corpus_idx + num_subtopics]\n",
    "        corpus_idx += num_subtopics\n",
    "        \n",
    "        # Map cluster IDs to their canonical names\n",
    "        conceptual_topics = {cluster_map[cid] for cid in model_cluster_ids}\n",
    "        \n",
    "        # Add to our data list for the final DataFrame\n",
    "        for topic in conceptual_topics:\n",
    "            krippendorff_data.append({\n",
    "                'docid': docid,\n",
    "                'model_name': model_name,\n",
    "                'conceptual_topic': topic\n",
    "            })\n",
    "            \n",
    "    return krippendorff_data, corpus\n",
    "\n",
    "\n",
    "# --- 3.1 TWO-STAGE EVALUATION FUNCTIONS ---\n",
    "\n",
    "def establish_cross_model_consensus_for_doc(doc_data, distance_threshold=0.7, n_clusters=None):\n",
    "    \"\"\"\n",
    "    STAGE 1: Establishes conceptual topics using ONLY cross-model data (excluding cir3).\n",
    "    This creates the \"ground truth\" consensus that we'll evaluate cir3 against.\n",
    "    \n",
    "    Returns:\n",
    "        consensus_data: List of dicts for Krippendorff's Alpha among cross-models\n",
    "        cluster_map: Mapping from cluster IDs to canonical topic names\n",
    "        corpus_embeddings: Embeddings of the cross-model corpus\n",
    "        clustering: The fitted clustering object for mapping new data\n",
    "    \"\"\"\n",
    "    docid = doc_data['docid']\n",
    "    cross_model_data = []\n",
    "    \n",
    "    # ONLY collect cross-model data (exclude cir3)\n",
    "    for cross_model in doc_data[\"perpectives\"][\"cross_model_classification\"]:\n",
    "        if cross_model[\"subtopics\"]:\n",
    "            cross_model_data.append(cross_model)\n",
    "    \n",
    "    if not cross_model_data:\n",
    "        return [], {}, np.array([]), None\n",
    "    \n",
    "    # Step 1: Create corpus from cross-models only\n",
    "    corpus = [subtopic for model_data in cross_model_data for subtopic in model_data['subtopics']]\n",
    "    if not corpus:\n",
    "        return [], {}, np.array([]), None\n",
    "    \n",
    "    corpus_embeddings = sentence_model.encode(corpus)\n",
    "    \n",
    "    # Step 2: Cluster cross-model subtopics to establish consensus concepts\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        distance_threshold=distance_threshold if n_clusters is None else None,\n",
    "        metric='cosine',\n",
    "        linkage='average'\n",
    "    ).fit(corpus_embeddings)\n",
    "    \n",
    "    cluster_labels = clustering.labels_\n",
    "    \n",
    "    # Step 3: Create canonical labels from cross-model consensus\n",
    "    cluster_map = {}\n",
    "    for cluster_id in set(cluster_labels):\n",
    "        indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "        cluster_subtopics = [corpus[i] for i in indices]\n",
    "        canonical_label = max(set(cluster_subtopics), key=cluster_subtopics.count)\n",
    "        cluster_map[cluster_id] = canonical_label\n",
    "    \n",
    "    # Step 4: Create Krippendorff's data for cross-models only\n",
    "    consensus_data = []\n",
    "    corpus_idx = 0\n",
    "    for model_data in cross_model_data:\n",
    "        model_name = model_data['model_name']\n",
    "        num_subtopics = len(model_data['subtopics'])\n",
    "        \n",
    "        model_cluster_ids = cluster_labels[corpus_idx : corpus_idx + num_subtopics]\n",
    "        corpus_idx += num_subtopics\n",
    "        \n",
    "        conceptual_topics = {cluster_map[cid] for cid in model_cluster_ids}\n",
    "        \n",
    "        for topic in conceptual_topics:\n",
    "            consensus_data.append({\n",
    "                'docid': docid,\n",
    "                'model_name': model_name,\n",
    "                'conceptual_topic': topic\n",
    "            })\n",
    "    \n",
    "    return consensus_data, cluster_map, corpus_embeddings, clustering\n",
    "\n",
    "\n",
    "def evaluate_cir3_against_consensus(doc_data, cluster_map, corpus_embeddings, clustering_obj, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    STAGE 2: Evaluates cir3's subtopics against the established cross-model consensus.\n",
    "    Uses semantic agreement score as the primary evaluation metric.\n",
    "    \n",
    "    Returns:\n",
    "        semantic_agreement_score: Primary metric for CIR3 performance evaluation\n",
    "        cir3_conceptual_topics: The consensus topics that cir3 identified\n",
    "        consensus_topics: All available consensus topics for this document\n",
    "    \"\"\"\n",
    "    docid = doc_data['docid']\n",
    "    cir3_subtopics = doc_data[\"perpectives\"][\"cir3_classifier_subtopics\"]\n",
    "    \n",
    "    if not cir3_subtopics or not cluster_map:\n",
    "        return 0.0, set(), set()\n",
    "    \n",
    "    # Get all consensus topics available for this document\n",
    "    consensus_topics = set(cluster_map.values())\n",
    "    \n",
    "    # Use the existing calculate_semantic_agreement function with soft-F1 method\n",
    "    # This provides a more nuanced evaluation than simple precision/recall\n",
    "    \n",
    "    # First, reconstruct the consensus subtopics for semantic comparison\n",
    "    consensus_subtopics = []\n",
    "    for cross_model in doc_data[\"perpectives\"][\"cross_model_classification\"]:\n",
    "        if cross_model[\"subtopics\"]:\n",
    "            consensus_subtopics.extend(cross_model[\"subtopics\"])\n",
    "    \n",
    "    if not consensus_subtopics:\n",
    "        return 0.0, set(), set()\n",
    "    \n",
    "    # Calculate semantic agreement using the soft-F1 method\n",
    "    # This is the primary metric for CIR3 evaluation\n",
    "    semantic_agreement_score = calculate_semantic_agreement(\n",
    "        cir3_subtopics, \n",
    "        consensus_subtopics, \n",
    "        similarity_threshold=similarity_threshold, \n",
    "        method=\"soft-f1\"\n",
    "    )\n",
    "    \n",
    "    # Also identify which conceptual topics CIR3 covered for interpretability\n",
    "    cir3_embeddings = sentence_model.encode(cir3_subtopics)\n",
    "    cir3_conceptual_topics = set()\n",
    "    \n",
    "    for i, cir3_embedding in enumerate(cir3_embeddings):\n",
    "        # Calculate similarity to all corpus embeddings\n",
    "        similarities = cosine_similarity([cir3_embedding], corpus_embeddings)[0]\n",
    "        \n",
    "        # Find the most similar subtopic in the corpus\n",
    "        best_match_idx = np.argmax(similarities)\n",
    "        best_similarity = similarities[best_match_idx]\n",
    "        \n",
    "        if best_similarity >= similarity_threshold:\n",
    "            # Find which cluster this subtopic belongs to\n",
    "            corpus_subtopics = []\n",
    "            for cross_model in doc_data[\"perpectives\"][\"cross_model_classification\"]:\n",
    "                if cross_model[\"subtopics\"]:\n",
    "                    corpus_subtopics.extend(cross_model[\"subtopics\"])\n",
    "            \n",
    "            if best_match_idx < len(corpus_subtopics):\n",
    "                # Find the cluster with the highest average similarity to this subtopic\n",
    "                best_cluster = None\n",
    "                max_avg_similarity = 0\n",
    "                \n",
    "                for cluster_id, canonical_name in cluster_map.items():\n",
    "                    # Calculate average similarity to all subtopics in this cluster\n",
    "                    cluster_similarities = []\n",
    "                    for idx, subtopic in enumerate(corpus_subtopics):\n",
    "                        if idx < len(corpus_embeddings):\n",
    "                            subtopic_sim = cosine_similarity(\n",
    "                                [corpus_embeddings[idx]], \n",
    "                                [sentence_model.encode([canonical_name])[0]]\n",
    "                            )[0][0]\n",
    "                            if subtopic_sim >= 0.4750:  # Only consider subtopics that belong to this cluster\n",
    "                                cluster_similarities.append(\n",
    "                                    cosine_similarity([corpus_embeddings[idx]], [cir3_embedding])[0][0]\n",
    "                                )\n",
    "                    \n",
    "                    if cluster_similarities:\n",
    "                        avg_similarity = np.mean(cluster_similarities)\n",
    "                        if avg_similarity > max_avg_similarity:\n",
    "                            max_avg_similarity = avg_similarity\n",
    "                            best_cluster = canonical_name\n",
    "                \n",
    "                if best_cluster:\n",
    "                    cir3_conceptual_topics.add(best_cluster)\n",
    "    \n",
    "    return semantic_agreement_score, cir3_conceptual_topics, consensus_topics\n",
    "\n",
    "\n",
    "def analyze_raw_cross_model_data(data_sample):\n",
    "    \"\"\"\n",
    "    Analyze the raw cross-model data to understand why models agree so much.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- RAW CROSS-MODEL DATA ANALYSIS ---\")\n",
    "    print(f\"Analyzing {len(data_sample)} documents\")\n",
    "    \n",
    "    all_subtopics_by_model = {}\n",
    "    overlap_matrix = {}\n",
    "    \n",
    "    for doc_data in data_sample:\n",
    "        docid = doc_data[\"docid\"]\n",
    "        print(f\"\\nDocument {docid}:\")\n",
    "        \n",
    "        # Collect subtopics by model for this document\n",
    "        doc_subtopics = {}\n",
    "        for cross_model in doc_data[\"perpectives\"][\"cross_model_classification\"]:\n",
    "            if cross_model[\"subtopics\"]:\n",
    "                model_name = cross_model[\"model_name\"]\n",
    "                subtopics = set(cross_model[\"subtopics\"])\n",
    "                doc_subtopics[model_name] = subtopics\n",
    "                print(f\"  {model_name}: {len(subtopics)} subtopics - {list(subtopics)[:3]}{'...' if len(subtopics) > 3 else ''}\")\n",
    "                \n",
    "                # Add to global collection\n",
    "                if model_name not in all_subtopics_by_model:\n",
    "                    all_subtopics_by_model[model_name] = set()\n",
    "                all_subtopics_by_model[model_name].update(subtopics)\n",
    "        \n",
    "        # Calculate pairwise overlap for this document\n",
    "        model_names = list(doc_subtopics.keys())\n",
    "        for i, model1 in enumerate(model_names):\n",
    "            for model2 in model_names[i+1:]:\n",
    "                set1 = doc_subtopics[model1]\n",
    "                set2 = doc_subtopics[model2]\n",
    "                if set1 and set2:\n",
    "                    jaccard = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "                    print(f\"  {model1} ↔ {model2}: {jaccard:.4f} Jaccard similarity\")\n",
    "    \n",
    "    # Global analysis\n",
    "    print(f\"\\n--- GLOBAL ANALYSIS ---\")\n",
    "    for model_name, subtopics in all_subtopics_by_model.items():\n",
    "        print(f\"{model_name}: {len(subtopics)} unique subtopics total\")\n",
    "    \n",
    "    # Global pairwise similarity\n",
    "    model_names = list(all_subtopics_by_model.keys())\n",
    "    print(f\"\\nGlobal Pairwise Jaccard Similarities:\")\n",
    "    for i, model1 in enumerate(model_names):\n",
    "        for model2 in model_names[i+1:]:\n",
    "            set1 = all_subtopics_by_model[model1]\n",
    "            set2 = all_subtopics_by_model[model2]\n",
    "            if set1 and set2:\n",
    "                jaccard = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "                print(f\"  {model1} ↔ {model2}: {jaccard:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_research_tables(pairwise_scores, cross_model_alpha, cir3_results, num_documents):\n",
    "    \"\"\"\n",
    "    Generate publication-ready tables for research paper.\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PUBLICATION-READY RESEARCH TABLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate CIR3 performance metrics\n",
    "    cir3_semantic_scores = [result['semantic_agreement_score'] for result in cir3_results]\n",
    "    \n",
    "    # Readable table format\n",
    "    print(\"\\n--- TABLE 1: Cross-Model Agreement Analysis ---\")\n",
    "    print(\"| Metric      | Mean  | Std Dev | Min   | Max   | n   |\")\n",
    "    print(\"|-------------|-------|---------|-------|-------|-----|\")\n",
    "    \n",
    "    for method_name, scores in pairwise_scores.items():\n",
    "        if scores:\n",
    "            mean_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "            min_score = np.min(scores)\n",
    "            max_score = np.max(scores)\n",
    "            n_scores = len(scores)\n",
    "            \n",
    "            formatted_method = method_name.replace('_', '-').title()\n",
    "            print(f\"| {formatted_method:<11} | {mean_score:.4f} | {std_score:.4f}   | {min_score:.4f} | {max_score:.4f} | {n_scores:<3} |\")\n",
    "    \n",
    "    print(f\"\\nNote: Agreement scores between CIR3 and cross-model classifications across {num_documents} documents.\")\n",
    "    print(\"- Jaccard: Set-based similarity\")\n",
    "    print(\"- Soft-F1: Semantic precision-recall\")  \n",
    "    print(\"- Bipartite: Hungarian algorithm matching\")\n",
    "    print(\"- Avg-Cosine: Average pairwise similarity\")\n",
    "    \n",
    "    # Table 2: Two-Stage Semantic Evaluation Results\n",
    "    print(\"\\n--- TABLE 2: Two-Stage Semantic Evaluation Results ---\")\n",
    "    print(\"| Metric                        | Value | Interpretation |\")\n",
    "    print(\"|-------------------------------|-------|----------------|\")\n",
    "    print(f\"| Cross-Model Reliability (α)   | {cross_model_alpha:.4f} | {'Excellent' if cross_model_alpha >= 0.8 else 'Good' if cross_model_alpha >= 0.67 else 'Moderate':<14} |\")\n",
    "    print(f\"| CIR3 Semantic Agreement (Mean) | {np.mean(cir3_semantic_scores):.4f} | {'Excellent' if np.mean(cir3_semantic_scores) >= 0.8 else 'Good' if np.mean(cir3_semantic_scores) >= 0.6 else 'Moderate':<14} |\")\n",
    "    print(f\"| CIR3 Semantic Agreement (Std Dev) | {np.std(cir3_semantic_scores):.4f} | {'Low variance' if np.std(cir3_semantic_scores) < 0.1 else 'Moderate variance':<14} |\")\n",
    "    print(f\"| Documents Evaluated           | {len(cir3_results):<5} | --             |\")\n",
    "    \n",
    "    print(f\"\\nNote: Two-stage evaluation process:\")\n",
    "    print(\"- Stage 1: Establish consensus from cross-models (Llama3-8B, GPT-4o-mini, Gemma-27B)\")\n",
    "    print(\"- Stage 2: Evaluate CIR3 alignment against consensus using semantic agreement score\")\n",
    "    print(\"- Krippendorff's α measures cross-model reliability\")\n",
    "    print(\"- Semantic Agreement Score uses soft-F1 method for nuanced evaluation\")\n",
    "    print(\"- Clustering threshold: 0.425 (cosine distance)\")\n",
    "    \n",
    "    # Table 3: Performance Distribution Analysis\n",
    "    # Bin the semantic agreement scores\n",
    "    semantic_bins = [(0.0, 0.6, \"Poor\"), (0.6, 0.8, \"Good\"), (0.8, 0.95, \"Excellent\"), (0.95, 1.0, \"Outstanding\")]\n",
    "    \n",
    "    \n",
    "    print(\"\\n--- TABLE 3: CIR3 Performance Distribution Analysis ---\")\n",
    "    print(\"| Performance Range | Semantic Agreement Range | Count | Percentage |\")\n",
    "    print(\"|-------------------|--------------------------|-------|------------|\")\n",
    "        \n",
    "    for min_val, max_val, label in semantic_bins:\n",
    "        if label == \"Outstanding\":\n",
    "            count = sum(1 for score in cir3_semantic_scores if min_val < score <= max_val)\n",
    "        else:\n",
    "            count = sum(1 for score in cir3_semantic_scores if min_val <= score < max_val)\n",
    "        percentage = (count / len(cir3_semantic_scores)) * 100 if cir3_semantic_scores else 0\n",
    "        print(f\"| {label:<17} | {min_val:.1f}--{max_val:.1f}               | {count:>5} | {percentage:>8.1f}% |\")\n",
    "    \n",
    "    print(f\"|-------------------|--------------------------|-------|------------|\")\n",
    "    print(f\"| {'Total':<17} | --                       | {len(cir3_semantic_scores):>5} | {'100.0':>8}% |\")\n",
    "    \n",
    "    print(f\"\\nPerformance Categories:\")\n",
    "    print(\"- Outstanding (>0.95): Near-perfect agreement\")\n",
    "    print(\"- Excellent (0.8-0.95): Strong agreement\")  \n",
    "    print(\"- Good (0.6-0.8): Reasonable agreement\")\n",
    "    print(\"- Poor (≤0.6): Weak agreement\")\n",
    "    \n",
    "    # Summary Statistics Table\n",
    "# --- 4. MAIN EVALUATION SCRIPT ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Part A: Pairwise Agreement Analysis (Original Method) ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PART A: PAIRWISE INTER-MODEL AGREEMENT ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    number_of_models = len(get_ai_settings().cross_classifier_agents_params)\n",
    "    evaluation_methods = [\"jaccard\", \"soft-f1\", \"bipartite\", \"avg-cosine\"]\n",
    "    all_scores_by_metric = {method: [] for method in evaluation_methods}\n",
    "\n",
    "\n",
    "    list_of_scores_bipartite = []\n",
    "    for doc_data in data:\n",
    "        docid = doc_data[\"docid\"]\n",
    "\n",
    "        list_of_scores_per_doc = []\n",
    "        \n",
    "        cir3_subtopics = doc_data[\"perpectives\"][\"cir3_classifier_subtopics\"]\n",
    "        print(f\"\\n--- Document ID: {docid} ---\")\n",
    "        \n",
    "        for cross_model_data in doc_data[\"perpectives\"][\"cross_model_classification\"]:\n",
    "            ref_model_name = cross_model_data[\"model_name\"]\n",
    "            ref_subtopics = cross_model_data[\"subtopics\"]\n",
    "            \n",
    "            print(f\"  Agreement with {ref_model_name}:\")\n",
    "            # score_sum_per_model = 0.0\n",
    "            for method_name in evaluation_methods:\n",
    "                score = calculate_semantic_agreement(cir3_subtopics, ref_subtopics, method=method_name)\n",
    "                all_scores_by_metric[method_name].append(score)\n",
    "                print(f\"    - {method_name.title():<12}: {score:.4f}\")\n",
    "                if method_name == \"bipartite\":\n",
    "                    list_of_scores_per_doc.append(score)\n",
    "            \n",
    "        list_of_scores_bipartite.append({\"docid\": docid, \"scores\": list_of_scores_per_doc, \"avg_score\": np.mean(list_of_scores_per_doc), \"method\": \"bipartite\"})\n",
    "\n",
    "    print(\"\\n--- Overall Pairwise Agreement Statistics ---\")\n",
    "    for method_name, scores in all_scores_by_metric.items():\n",
    "        if scores:\n",
    "            avg_score = np.mean(scores)\n",
    "            std_dev = np.std(scores)\n",
    "            print(f\"  {method_name.title():<12} | Average: {avg_score:.4f}, Std Dev: {std_dev:.4f}\")\n",
    "            \n",
    "    # Paired T-tests (Comparing Metrics)\n",
    "    print(\"\\n--- Paired T-Tests: Comparing Agreement Metrics ---\")\n",
    "    metrics_to_compare_pairs = list(itertools.combinations(evaluation_methods, 2))\n",
    "    for m1, m2 in metrics_to_compare_pairs:\n",
    "        scores1 = all_scores_by_metric[m1]\n",
    "        scores2 = all_scores_by_metric[m2]\n",
    "        if len(scores1) > 1 and len(scores1) == len(scores2):\n",
    "            stat, p_value = ttest_rel(scores1, scores2)\n",
    "            print(f\"\\n  Comparing {m1.title()} vs {m2.title()}:\")\n",
    "            print(f\"    T-statistic: {stat:.4f}, P-value: {p_value:.4f}\")\n",
    "            if p_value < 0.05:\n",
    "                print(\"    Result: Statistically significant difference.\")\n",
    "            else:\n",
    "                print(\"    Result: No statistically significant difference.\")\n",
    "\n",
    "    \n",
    "    # --- Part B: Two-Stage Semantic Evaluation ---\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"PART B: TWO-STAGE SEMANTIC EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    CLUSTERING_DISTANCE_THRESHOLD = 0.4750  # Further reduced to 0.4 for more restrictive clustering\n",
    "    print(f\"\\nClustering subtopics with distance threshold: {CLUSTERING_DISTANCE_THRESHOLD}\")\n",
    "    print(\"⚠ Note: Reduced threshold to 0.425 for more fine-grained clustering and specific topic separation\")\n",
    "    \n",
    "    # DIAGNOSTIC: Analyze raw cross-model data first\n",
    "    analyze_raw_cross_model_data(data[:5])  # Analyze first 5 documents in detail\n",
    "    \n",
    "    # STAGE 1: Establish cross-model consensus\n",
    "    print(\"\\n--- STAGE 1: Establishing Cross-Model Consensus ---\")\n",
    "    cross_model_consensus_data = []\n",
    "    all_consensus_corpus = []\n",
    "    cir3_evaluation_results = []\n",
    "    \n",
    "    for doc_data in data:\n",
    "        docid = doc_data[\"docid\"]\n",
    "        \n",
    "        # Establish consensus from cross-models only\n",
    "        consensus_data, cluster_map, corpus_embeddings, clustering_obj = establish_cross_model_consensus_for_doc(\n",
    "            doc_data=doc_data,\n",
    "            distance_threshold=CLUSTERING_DISTANCE_THRESHOLD,\n",
    "            n_clusters=None  # Use distance threshold\n",
    "        )\n",
    "        \n",
    "        cross_model_consensus_data.extend(consensus_data)\n",
    "        \n",
    "        # Collect corpus for visualization\n",
    "        if len(consensus_data) > 0:\n",
    "            cross_model_subtopics = []\n",
    "            for cross_model in doc_data[\"perpectives\"][\"cross_model_classification\"]:\n",
    "                if cross_model[\"subtopics\"]:\n",
    "                    cross_model_subtopics.extend(cross_model[\"subtopics\"])\n",
    "            all_consensus_corpus.extend(cross_model_subtopics)\n",
    "        \n",
    "        # STAGE 2: Evaluate cir3 against this consensus\n",
    "        semantic_agreement_score, cir3_topics, consensus_topics = evaluate_cir3_against_consensus(\n",
    "            doc_data=doc_data,\n",
    "            cluster_map=cluster_map,\n",
    "            corpus_embeddings=corpus_embeddings,\n",
    "            clustering_obj=clustering_obj,\n",
    "            similarity_threshold=0.7\n",
    "        )\n",
    "        \n",
    "        cir3_evaluation_results.append({\n",
    "            'docid': docid,\n",
    "            'semantic_agreement_score': semantic_agreement_score,\n",
    "            'cir3_topics': cir3_topics,\n",
    "            'consensus_topics': consensus_topics\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nDocument {docid}:\")\n",
    "        print(f\"  Consensus Topics: {consensus_topics}\")\n",
    "        print(f\"  CIR3 Topics: {cir3_topics}\")\n",
    "        print(f\"  CIR3 Semantic Agreement Score: {semantic_agreement_score:.4f}\")\n",
    "    \n",
    "    # Calculate cross-model consensus reliability (Krippendorff's Alpha)\n",
    "    if cross_model_consensus_data:\n",
    "        consensus_df = pd.DataFrame(cross_model_consensus_data)\n",
    "        print(f\"\\n--- Cross-Model Consensus Reliability ---\")\n",
    "        print(f\"Sample of consensus data:\")\n",
    "        print(consensus_df.head())\n",
    "        \n",
    "        # DIAGNOSTIC: Check clustering results\n",
    "        print(f\"\\n--- CLUSTERING DIAGNOSTICS ---\")\n",
    "        unique_topics = consensus_df['conceptual_topic'].unique()\n",
    "        print(f\"Number of unique conceptual topics: {len(unique_topics)}\")\n",
    "        print(f\"Conceptual topics: {unique_topics}\")\n",
    "        \n",
    "        # Check distribution of topics per model\n",
    "        topic_counts = consensus_df.groupby('model_name')['conceptual_topic'].nunique()\n",
    "        print(f\"\\nTopics per model:\")\n",
    "        for model, count in topic_counts.items():\n",
    "            print(f\"  {model}: {count} topics\")\n",
    "            \n",
    "        # Check if all models have the same topics (indicating over-clustering)\n",
    "        model_topic_sets = {}\n",
    "        for model in consensus_df['model_name'].unique():\n",
    "            model_topics = set(consensus_df[consensus_df['model_name'] == model]['conceptual_topic'].unique())\n",
    "            model_topic_sets[model] = model_topics\n",
    "            print(f\"  {model} topics: {model_topics}\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            cross_model_alpha = simpledorff.calculate_krippendorffs_alpha_for_df(\n",
    "                consensus_df,\n",
    "                experiment_col='docid',\n",
    "                annotator_col='model_name',\n",
    "                class_col='conceptual_topic'\n",
    "            )\n",
    "            print(f\"\\nCross-Model Krippendorff's Alpha: {cross_model_alpha:.4f}\")\n",
    "            \n",
    "            # Set alpha_score for compatibility with existing summary code\n",
    "            alpha_score = cross_model_alpha\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate cross-model Alpha: {e}\")\n",
    "            alpha_score = 0.0\n",
    "    else:\n",
    "        alpha_score = 0.0\n",
    "    \n",
    "    # Analyze CIR3's performance against consensus\n",
    "    print(f\"\\n--- CIR3 Performance Against Cross-Model Consensus ---\")\n",
    "    semantic_agreement_scores = [result['semantic_agreement_score'] for result in cir3_evaluation_results]\n",
    "    \n",
    "    if semantic_agreement_scores:\n",
    "        print(f\"CIR3 Semantic Agreement Statistics:\")\n",
    "        print(f\"  Average Semantic Agreement Score: {np.mean(semantic_agreement_scores):.4f}\")\n",
    "        print(f\"  Std Dev Semantic Agreement Score: {np.std(semantic_agreement_scores):.4f}\")\n",
    "        print(f\"  Min Semantic Agreement Score: {np.min(semantic_agreement_scores):.4f}\")\n",
    "        print(f\"  Max Semantic Agreement Score: {np.max(semantic_agreement_scores):.4f}\")\n",
    "        \n",
    "        # Performance interpretation\n",
    "        avg_semantic_score = np.mean(semantic_agreement_scores)\n",
    "        print(f\"\\n  CIR3 Performance Interpretation:\")\n",
    "        if avg_semantic_score >= 0.8:\n",
    "            print(\"  Excellent: CIR3 strongly aligns with cross-model consensus\")\n",
    "        elif avg_semantic_score >= 0.6:\n",
    "            print(\"  Good: CIR3 reasonably aligns with cross-model consensus\")\n",
    "        elif avg_semantic_score >= 0.4:\n",
    "            print(\"  Moderate: CIR3 has partial alignment with cross-model consensus\")\n",
    "        else:\n",
    "            print(\"  Poor: CIR3 weakly aligns with cross-model consensus\")\n",
    "    \n",
    "    # Visualization of consensus\n",
    "    print(f\"\\n--- Consensus Visualization ---\")\n",
    "    if all_consensus_corpus:\n",
    "        print(f\"Plotting dendrogram for {len(all_consensus_corpus)} consensus subtopics...\")\n",
    "        plot_dendrogram(all_consensus_corpus)\n",
    "        \n",
    "        # Also create corpus_list for compatibility with existing code\n",
    "        corpus_list = all_consensus_corpus\n",
    "    else:\n",
    "        corpus_list = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TWO-STAGE ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    generate_research_tables(\n",
    "        pairwise_scores=all_scores_by_metric,\n",
    "        cross_model_alpha=alpha_score,\n",
    "        cir3_results=cir3_evaluation_results,\n",
    "        num_documents=len(data)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n \\n \\n ----------------- RESTRUCTURE ALL SCORES ------------------\")\n",
    "\n",
    "# [0, 0.33] ]0.33, 0.66] ]0.66, 0.75]  ]0.75, 1]  \n",
    "list_of_bins_0_50 = []\n",
    "list_of_bins_50_066 = []\n",
    "list_of_bins_066_075 = []\n",
    "list_of_bins_075_1 = []\n",
    "\n",
    "for item in list_of_scores_bipartite:\n",
    "    if item[\"avg_score\"] <= 0.50:\n",
    "        list_of_bins_0_50.append(item)\n",
    "    elif item[\"avg_score\"] <= 0.66:\n",
    "        list_of_bins_50_066.append(item)\n",
    "    elif item[\"avg_score\"] <= 0.75:\n",
    "        list_of_bins_066_075.append(item)\n",
    "    else:\n",
    "        list_of_bins_075_1.append(item)\n",
    "\n",
    "print(len(list_of_bins_0_50))\n",
    "print(len(list_of_bins_50_066))\n",
    "print(len(list_of_bins_066_075))\n",
    "print(len(list_of_bins_075_1))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define intervals and corresponding lists\n",
    "intervals = [\n",
    "    \"[0, 0.50]\",\n",
    "    \"]0.50, 0.66]\",\n",
    "    \"]0.66, 0.75]\",\n",
    "    \"]0.75, 1]\"\n",
    "]\n",
    "lists = [\n",
    "    list_of_bins_0_50,\n",
    "    list_of_bins_50_066,\n",
    "    list_of_bins_066_075,\n",
    "    list_of_bins_075_1\n",
    "]\n",
    "\n",
    "# Print table header\n",
    "print(f\"{'Interval':<15} {'Count':<10} {'Avg':<10} {'Median':<10} {'Min':<10} {'Max':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "\n",
    "count_list = []\n",
    "avg_scores_list = []\n",
    "median_scores_list = []\n",
    "min_scores_list = []\n",
    "max_scores_list = []\n",
    "\n",
    "# Print each row\n",
    "for interval, bin_list in zip(intervals, lists):\n",
    "    scores = [item[\"avg_score\"] for item in bin_list]\n",
    "    count = len(scores)\n",
    "    count_list.append(count)\n",
    "    avg_score = np.mean(scores) if scores else 0\n",
    "    avg_scores_list.append(avg_score)\n",
    "    median_score = np.median(scores) if scores else 0\n",
    "    median_scores_list.append(median_score)\n",
    "    min_score = np.min(scores) if scores else 0\n",
    "    min_scores_list.append(min_score)\n",
    "    max_score = np.max(scores) if scores else 0\n",
    "    max_scores_list.append(max_score)\n",
    "    print(f\"{interval:<15} {count:<10} {avg_score:<10.4f} {median_score:<10.4f} {min_score:<10.4f} {max_score:<10.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n============== Overall Pairwise Agreement Statistics ==============\")\n",
    "for method_name, scores in all_scores_by_metric.items():\n",
    "    if scores:\n",
    "        avg_score = np.mean(scores)\n",
    "        std_dev = np.std(scores)\n",
    "        print(f\"  {method_name.title():<12} | Average: {avg_score:.4f}, Std Dev: {std_dev:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n============== Krippendorff's Alpha Result ==============\")\n",
    "print(f\"  Alpha Score: {alpha_score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cir3_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
