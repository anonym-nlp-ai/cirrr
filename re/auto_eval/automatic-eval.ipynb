{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cfc6fc",
   "metadata": {},
   "source": [
    "# Automatic Evaluation on a few samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c387652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ai\\workspace\\PhD\\cir3_anonym\\cirrr\n",
      " Volume in drive C is OS\n",
      " Volume Serial Number is BE2B-F904\n",
      "\n",
      " Directory of c:\\ai\\workspace\\PhD\\cir3_anonym\\cirrr\n",
      "\n",
      "16/06/2024  14:46    <DIR>          .\n",
      "16/06/2024  02:48    <DIR>          ..\n",
      "16/06/2024  14:51                80 .env\n",
      "16/06/2024  05:14             3,369 .gitignore\n",
      "16/06/2024  14:46    <DIR>          aimw\n",
      "16/06/2024  05:09    <DIR>          conf\n",
      "16/06/2024  14:51             1,432 docker-compose.yml\n",
      "16/06/2024  15:55    <DIR>          re\n",
      "16/06/2024  02:51                 6 README.md\n",
      "               4 File(s)          4,887 bytes\n",
      "               5 Dir(s)  326,002,257,920 bytes free\n"
     ]
    }
   ],
   "source": [
    "# cd to \"Directory of \"./alignment\"\n",
    "%cd ../../\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ff9a6d2-29f2-434d-906c-ecba3839b6ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "# !conda activate phd-align-py3-11\n",
    "import torch\n",
    "\n",
    "logger.info(torch.cuda.is_available())\n",
    "logger.info(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e616a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\n",
    "    \"docid\": 54251,\n",
    "    \"doc\": \"There are two types of credit checks. First is the hard pull which is typically done when you apply for a credit line. The lender will hard pull your file and make his/her decision based on that. This affects your score negatively. You might lose few points for one hard inquiry. Second type is soft pull, which is done as a background check. Typically done by credit card companies to send you a pre-approved offer, or renting an apartment etc. This does not affect your score. One thing to keep in mind is a company will not do a hard pull without your permission, where as they can do soft pulls without you even knowing.  Soft inquiries vs hard inquiries\",\n",
    "    \"tokenized_size\": 140.0,\n",
    "    \"max_seq_len_exceeded\": False,\n",
    "    \"cct_saar\": {\n",
    "        \"queries_aspects\": [\n",
    "            {\n",
    "                \"question\": \"What are the two types of credit checks?\",\n",
    "                \"answer\": \"There are two types of credit checks: hard pull and soft pull. A hard pull is typically done when you apply for a credit line, while a soft pull is done as a background check.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"How does a hard pull affect your credit score?\",\n",
    "                \"answer\": \"A hard pull can negatively affect your credit score, causing you to lose a few points.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the difference between a hard pull and a soft pull?\",\n",
    "                \"answer\": \"A hard pull is a more invasive check that affects your credit score, while a soft pull is a background check that does not affect your score.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Can a company do a hard pull without your permission?\",\n",
    "                \"answer\": \"No, a company cannot do a hard pull without your permission.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the purpose of a soft pull?\",\n",
    "                \"answer\": \"A soft pull is typically done as a background check, such as when a credit card company sends you a pre-approved offer or when you're renting an apartment.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the main difference between a hard pull and a soft pull?\",\n",
    "                \"answer\": \"The main difference is that a hard pull affects your credit score, while a soft pull does not.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the impact of a hard pull on your credit score?\",\n",
    "                \"answer\": \"A hard pull can cause a temporary decrease in your credit score, but the impact is usually minor.\",\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f8226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimw.app.utils.json_utils import JSONProcessor\n",
    "\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5780eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dir = \"./re/data/fiqa/clean/base/exp_001/\"\n",
    "w_dir = \"\"\n",
    "jSONProcessor = JSONProcessor(r_dir + \"cct_saar_corpus_cln_split_0.json\")\n",
    "data = jSONProcessor.read_json_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eafb2e",
   "metadata": {},
   "source": [
    "## SentenceTransformer Using `BAAI/bge-large-en-v1.5`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3074ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "\n",
    "def get_average_scores(data):\n",
    "    mean_of_mean_score_context_vs_queries = 0.0\n",
    "    mean_of_mean_score_context_vs_queries_joint = 0.0\n",
    "    mean_of_mean_score_context_vs_answers = 0.0\n",
    "    mean_of_mean_score_context_vs_answers_joint = 0.0\n",
    "    mean_of_mean_score_questions_answers = 0.0\n",
    "    mean_of_mean_score_questions_joint_vs_answers_joint = 0.0\n",
    "    mean_of_mean_score_questions_joint_and_answers_joint_vs_context = 0.0\n",
    "    mean_of_mean_score_questions_and_answers_vs_context = 0.0\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    number_of_pairs = len(answers)\n",
    "                    # Embed targets\n",
    "                    context_embedding = model.encode(context, normalize_embeddings=True)\n",
    "                    queries_embeddings = model.encode(\n",
    "                        queries, normalize_embeddings=True\n",
    "                    )\n",
    "                    queries_joint_embeddings = model.encode(\n",
    "                        queries_joint, normalize_embeddings=True\n",
    "                    )\n",
    "                    answers_embeddings = model.encode(\n",
    "                        answers, normalize_embeddings=True\n",
    "                    )\n",
    "                    answers_joint_embeddings = model.encode(\n",
    "                        answers_joint, normalize_embeddings=True\n",
    "                    )\n",
    "\n",
    "                    # Compute similarity scores\n",
    "\n",
    "                    ############# context Vs queries #############\n",
    "                    similarity_context_queries = (\n",
    "                        context_embedding @ queries_embeddings.T\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarity_context_queries: {similarity_context_queries}\"\n",
    "                    )\n",
    "                    mean_score_context_vs_queries = sum(\n",
    "                        similarity_context_queries\n",
    "                    ) / len(\n",
    "                        similarity_context_queries\n",
    "                    )  # <----------\n",
    "                    mean_of_mean_score_context_vs_queries = (\n",
    "                        mean_of_mean_score_context_vs_queries\n",
    "                        + mean_score_context_vs_queries\n",
    "                    )\n",
    "\n",
    "                    ############# context Vs concatenated queries #############\n",
    "                    similarity_context_queries_joint = (\n",
    "                        context_embedding @ queries_joint_embeddings.T\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarity_context_queries_joint: {similarity_context_queries_joint}\"\n",
    "                    )\n",
    "                    mean_of_mean_score_context_vs_queries_joint = (\n",
    "                        mean_of_mean_score_context_vs_queries_joint\n",
    "                        + similarity_context_queries_joint\n",
    "                    )\n",
    "\n",
    "                    ############# context Vs answers #############\n",
    "                    similarity_context_answers = (\n",
    "                        context_embedding @ answers_embeddings.T\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarity_context_answers: {similarity_context_answers}\"\n",
    "                    )\n",
    "                    mean_score_context_vs_answers = sum(\n",
    "                        similarity_context_answers\n",
    "                    ) / len(\n",
    "                        similarity_context_answers\n",
    "                    )  # <----------\n",
    "                    mean_of_mean_score_context_vs_answers = (\n",
    "                        mean_of_mean_score_context_vs_answers\n",
    "                        + mean_score_context_vs_answers\n",
    "                    )\n",
    "                    ############# context Vs concatenated answers #############\n",
    "                    similarity_context_answers_joint = (\n",
    "                        context_embedding @ answers_joint_embeddings.T\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarity_context_answers_joint: {similarity_context_answers_joint}\"\n",
    "                    )\n",
    "                    mean_of_mean_score_context_vs_answers_joint = (\n",
    "                        mean_of_mean_score_context_vs_answers_joint\n",
    "                        + similarity_context_answers_joint\n",
    "                    )\n",
    "\n",
    "                    ################ Questions Vs Answers ################\n",
    "                    similarity_questions_answers = [\n",
    "                        x @ y.T for x, y in zip(answers_embeddings, queries_embeddings)\n",
    "                    ]\n",
    "                    mean_score_questions_answers = sum(\n",
    "                        similarity_questions_answers\n",
    "                    ) / len(\n",
    "                        similarity_questions_answers\n",
    "                    )  # <-----------\n",
    "                    mean_of_mean_score_questions_answers = (\n",
    "                        mean_of_mean_score_questions_answers\n",
    "                        + mean_score_questions_answers\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarty scores: {similarity_questions_answers} | average similarity score: {mean_score_questions_answers}\"\n",
    "                    )\n",
    "\n",
    "                    ################ Questions Joint Vs Answers Joint ################\n",
    "                    mean_score_questions_joint_vs_answers_joint = (\n",
    "                        answers_joint_embeddings @ queries_joint_embeddings.T\n",
    "                    )  # <----------\n",
    "                    mean_of_mean_score_questions_joint_vs_answers_joint = (\n",
    "                        mean_of_mean_score_questions_joint_vs_answers_joint\n",
    "                        + mean_score_questions_joint_vs_answers_joint\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarty scores of concatenated Queries Vs Answers: {mean_score_questions_joint_vs_answers_joint}\"\n",
    "                    )\n",
    "\n",
    "                    ################ (Joint Questions + Joint Answers)/2 Vs Context ################\n",
    "                    mean_score_questions_joint_and_answers_joint_vs_context = (\n",
    "                        context_embedding\n",
    "                        @ ((answers_joint_embeddings + queries_joint_embeddings) / 2).T\n",
    "                    )  # <----------\n",
    "                    mean_of_mean_score_questions_joint_and_answers_joint_vs_context = (\n",
    "                        mean_of_mean_score_questions_joint_and_answers_joint_vs_context\n",
    "                        + mean_score_questions_joint_and_answers_joint_vs_context\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarty scores of concatenated Answers embeddings + concatenated Queries embeddings Vs Context: {mean_score_questions_joint_and_answers_joint_vs_context}\"\n",
    "                    )\n",
    "\n",
    "                    ################ (Questions/n + Answers/n)/2 Answers) Vs Context ################\n",
    "                    mean_answers_embeddings = sum(answers_embeddings) / len(\n",
    "                        answers_embeddings\n",
    "                    )\n",
    "                    mean_questions_embeddings = sum(queries_embeddings) / len(\n",
    "                        queries_embeddings\n",
    "                    )\n",
    "                    mean_score_questions_and_answers_vs_context = (\n",
    "                        context_embedding\n",
    "                        @ ((mean_answers_embeddings + mean_questions_embeddings) / 2).T\n",
    "                    )  # <----------\n",
    "                    mean_of_mean_score_questions_and_answers_vs_context = (\n",
    "                        mean_of_mean_score_questions_and_answers_vs_context\n",
    "                        + mean_score_questions_and_answers_vs_context\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarty scores of mean answers,questions embeddings Vs Context: {mean_score_questions_and_answers_vs_context}\"\n",
    "                    )\n",
    "\n",
    "    return (\n",
    "        eval_data_size,\n",
    "        (\n",
    "            mean_of_mean_score_context_vs_queries / eval_data_size,\n",
    "            mean_of_mean_score_context_vs_queries_joint / eval_data_size,\n",
    "            mean_of_mean_score_context_vs_answers / eval_data_size,\n",
    "            mean_of_mean_score_context_vs_answers_joint / eval_data_size,\n",
    "            mean_of_mean_score_questions_answers / eval_data_size,\n",
    "            mean_of_mean_score_questions_joint_vs_answers_joint / eval_data_size,\n",
    "            mean_of_mean_score_questions_joint_and_answers_joint_vs_context\n",
    "            / eval_data_size,\n",
    "            mean_of_mean_score_questions_and_answers_vs_context / eval_data_size,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# mean_of_mean_score_context_vs_queries = mean_of_mean_score_context_vs_queries / eval_data_size\n",
    "# mean_of_mean_score_context_vs_queries_joint = mean_of_mean_score_context_vs_queries_joint / eval_data_size\n",
    "# mean_of_mean_score_context_vs_answers = mean_of_mean_score_context_vs_answers / eval_data_size\n",
    "# mean_of_mean_score_context_vs_answers_joint = mean_of_mean_score_context_vs_answers_joint / eval_data_size\n",
    "# mean_of_mean_score_questions_answers = mean_of_mean_score_questions_answers / eval_data_size\n",
    "# mean_of_mean_score_questions_joint_vs_answers_joint = mean_of_mean_score_questions_joint_vs_answers_joint / eval_data_size\n",
    "# mean_of_mean_score_questions_joint_and_answers_joint_vs_context = mean_of_mean_score_questions_joint_and_answers_joint_vs_context / eval_data_size\n",
    "# mean_of_mean_score_questions_and_answers_vs_context = mean_of_mean_score_questions_and_answers_vs_context / eval_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3c60e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:02:06.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:07.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1msimilarity_context_queries: [0.52898943 0.68211067 0.58506644 0.53367245 0.5756325  0.76060915\n",
      " 0.5653718  0.5762904  0.54499257 0.45893148]\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:07.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1msimilarity_context_queries_joint: 0.7120344042778015\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:07.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1msimilarity_context_answers: [0.5467243  0.61753774 0.58939314 0.57899636 0.6150774  0.697689\n",
      " 0.53532606 0.6085215  0.53775287 0.58703613]\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:07.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1msimilarity_context_answers_joint: 0.7611955404281616\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:07.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1msimilarty scores: [0.89411324, 0.7722802, 0.8500412, 0.85180783, 0.84971434, 0.82390594, 0.8183195, 0.88741446, 0.8092735, 0.6947276] | average similarity score: 0.8251597821712494\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:07.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1msimilarty scores of concatenated Queries Vs Answers: 0.9057860374450684\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:07.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1msimilarty scores of concatenated Answers embeddings + concatenated Queries embeddings Vs Context: 0.7366149425506592\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:07.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1msimilarty scores of mean answers,questions embeddings Vs Context: 0.5862860679626465\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:07.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1msimilarity_context_queries: [0.7016512  0.73209006 0.71189153 0.75363326 0.5178586  0.69601846\n",
      " 0.7117703  0.43657258 0.69866633 0.52545065 0.6875121 ]\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1msimilarity_context_queries_joint: 0.7592473030090332\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1msimilarity_context_answers: [0.6978517  0.6923562  0.6659051  0.6121842  0.63499665 0.69063395\n",
      " 0.6674482  0.6769841  0.6160875  0.6445549  0.7016629 ]\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1msimilarity_context_answers_joint: 0.7983558773994446\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1msimilarty scores: [0.79352665, 0.8408966, 0.7434759, 0.73539555, 0.48650452, 0.87577975, 0.64837736, 0.4057243, 0.7414546, 0.635416, 0.82299703] | average similarity score: 0.7026862041516737\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1msimilarty scores of concatenated Queries Vs Answers: 0.8485373258590698\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1msimilarty scores of concatenated Answers embeddings + concatenated Queries embeddings Vs Context: 0.7788015604019165\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1msimilarty scores of mean answers,questions embeddings Vs Context: 0.6578992009162903\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1msimilarity_context_queries: [0.6488985  0.8864128  0.74058026 0.8565353  0.6617228  0.80300856\n",
      " 0.76725495 0.70381707 0.63030636 0.7580767  0.4405538 ]\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1msimilarity_context_queries_joint: 0.8973995447158813\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1msimilarity_context_answers: [0.6847426  0.7665308  0.69372755 0.70219654 0.6616186  0.7152645\n",
      " 0.75728893 0.61210525 0.6946394  0.72398263 0.6665436 ]\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1msimilarity_context_answers_joint: 0.8409755825996399\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1msimilarty scores: [0.815304, 0.80861825, 0.93141925, 0.6777586, 0.79070956, 0.75230503, 0.86260504, 0.81152105, 0.8049307, 0.9309794, 0.71762466] | average similarity score: 0.8094341321425005\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1msimilarty scores of concatenated Queries Vs Answers: 0.8878791332244873\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1msimilarty scores of concatenated Answers embeddings + concatenated Queries embeddings Vs Context: 0.869187593460083\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1msimilarty scores of mean answers,questions embeddings Vs Context: 0.7079912424087524\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1msimilarity_context_queries: [0.7670344  0.67655414 0.7701797  0.6850603  0.753701   0.7325296\n",
      " 0.71515316 0.7526037  0.753701   0.7325296  0.71515316]\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1msimilarity_context_queries_joint: 0.7830677032470703\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1msimilarity_context_answers: [0.8592243  0.69165057 0.7843487  0.65409684 0.75638145 0.69880044\n",
      " 0.6456345  0.6935111  0.7212126  0.7189024  0.6456345 ]\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1msimilarity_context_answers_joint: 0.837113618850708\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1msimilarty scores: [0.8426938, 0.8490411, 0.7964607, 0.6811689, 0.8517766, 0.85556626, 0.83825004, 0.8681148, 0.8933158, 0.8872951, 0.83825004] | average similarity score: 0.8365393822843378\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1msimilarty scores of concatenated Queries Vs Answers: 0.8804562091827393\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1msimilarty scores of concatenated Answers embeddings + concatenated Queries embeddings Vs Context: 0.8100906610488892\u001b[0m\n",
      "\u001b[32m2024-06-16 16:02:08.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_scores\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1msimilarty scores of mean answers,questions embeddings Vs Context: 0.7237999439239502\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " (0.6708480805835941,\n",
       "  0.7879372388124466,\n",
       "  0.6671400761062448,\n",
       "  0.8094101548194885,\n",
       "  0.7934548751874403,\n",
       "  0.8806646764278412,\n",
       "  0.798673689365387,\n",
       "  0.6689941138029099))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_scores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d52a8",
   "metadata": {},
   "source": [
    "## BERTScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b57552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score\n",
    "from bert_score import score\n",
    "\n",
    "bert_score.__version__\n",
    "\n",
    "# hide the loading messages\n",
    "import logging\n",
    "import transformers\n",
    "\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def get_average_BERTscores(data):\n",
    "    mean_P_aj_c, mean_R_aj_c, mean_F1_aj_c = 0.0, 0.0, 0.0\n",
    "    mean_P_qj_c, mean_R_qj_c, mean_F1_qj_c = 0.0, 0.0, 0.0\n",
    "    mean_P_qj_aj, mean_R_qj_aj, mean_F1_qj_aj = 0.0, 0.0, 0.0\n",
    "    eval_data_size = 0\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    # Between Answers\n",
    "                    P_aj_c, R_aj_c, F1_aj_c = score(\n",
    "                        [answers_joint], [context], lang=\"en\", verbose=True\n",
    "                    )\n",
    "                    P_qj_c, R_qj_c, F1_qj_c = score(\n",
    "                        [queries_joint], [context], lang=\"en\", verbose=True\n",
    "                    )\n",
    "                    P_qj_aj, R_qj_aj, F1_qj_aj = score(\n",
    "                        [queries_joint], [answers_joint], lang=\"en\", verbose=True\n",
    "                    )\n",
    "\n",
    "                    mean_P_aj_c = mean_P_aj_c + P_aj_c\n",
    "                    mean_R_aj_c = mean_R_aj_c + R_aj_c\n",
    "                    mean_F1_aj_c = mean_F1_aj_c + F1_aj_c\n",
    "                    mean_P_qj_c = mean_P_qj_c + P_qj_c\n",
    "                    mean_R_qj_c = mean_R_qj_c + R_qj_c\n",
    "                    mean_F1_qj_c = mean_F1_qj_c + F1_qj_c\n",
    "                    mean_P_qj_aj = mean_P_qj_aj + P_qj_aj\n",
    "                    mean_R_qj_aj = mean_R_qj_aj + R_qj_aj\n",
    "                    mean_F1_qj_aj = mean_F1_qj_aj + F1_qj_aj\n",
    "\n",
    "    return (\n",
    "        eval_data_size,\n",
    "        (\n",
    "            mean_P_aj_c / eval_data_size,\n",
    "            mean_R_aj_c / eval_data_size,\n",
    "            mean_F1_aj_c / eval_data_size,\n",
    "        ),\n",
    "        (\n",
    "            mean_P_qj_c / eval_data_size,\n",
    "            mean_R_qj_c / eval_data_size,\n",
    "            mean_F1_qj_c / eval_data_size,\n",
    "        ),\n",
    "        (\n",
    "            mean_P_qj_aj / eval_data_size,\n",
    "            mean_R_qj_aj / eval_data_size,\n",
    "            mean_F1_qj_aj / eval_data_size,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3692e6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:04:53.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_BERTscores\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483a875768674f2297d37c31fb8f30b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f5af7a67f54daeaa14a412f4f2f40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.22 seconds, 4.57 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec2e3739feb4dd7aa0c27e115179bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdf531e209f4dfbaf74cef670f81e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.04 seconds, 25.01 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01623ec762004a5697572a55fddc590e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a268a0c961374017b10e3d68a0467b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:04:58.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_BERTscores\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.10 seconds, 10.39 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa973c79aec4c35acfb42b771fb5512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee43f5b1135e471a9a161f0e99fb0a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.07 seconds, 13.62 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9920804a7ab04a7981e7b3eb7860fb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4a78001a6242489d1533427c7fd295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.06 seconds, 17.36 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463af83876bf4319be7507d974aa88f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed06d46b273049d5b904425516c801e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:05:01.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_BERTscores\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.11 seconds, 9.18 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514e6bdd26504735ad39df1149a39893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0393ab460f403e879c45360c7b9622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.09 seconds, 11.69 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9affc268bd4d92ae7c10bbae26ba28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afa34d1e7734fa3a35f160b34a19c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.05 seconds, 22.15 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ecd04d5f0b447fa447fb4438716a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d125d7fd5014712bf689f24434b3c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:05:04.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_BERTscores\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.07 seconds, 13.56 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08d9fdf618645c696971b1c402ea8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5165f1a2bea841d1b78a293b8ea001ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.09 seconds, 11.46 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44da01836024bd28d9b0c27f4539a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e9a126a4c647118fab8482109badf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.05 seconds, 21.17 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b07bb2a931f495c83ce686e9f69437b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687aa3e9f6d843d38888cc51a97bcdbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.09 seconds, 11.65 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " (tensor([0.8320]), tensor([0.8679]), tensor([0.8495])),\n",
       " (tensor([0.8497]), tensor([0.8631]), tensor([0.8563])),\n",
       " (tensor([0.8910]), tensor([0.8522]), tensor([0.8712])))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_BERTscores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed486cf",
   "metadata": {},
   "source": [
    "## Vendi Score for diversity\n",
    "\n",
    "If diversity between concatned answers and the context is low (close to 1), this means comprehensivness and less hallucination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d692ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vendi_score import text_utils\n",
    "\n",
    "\n",
    "def get_average_VENDIscores(data):\n",
    "    mean_ngram_vs_c_aj = 0.0\n",
    "    mean_ngram_vs_c_qj = 0.0\n",
    "    mean_ngram_vs_qj_aj = 0.0\n",
    "    mean_bert_vs_c_aj = 0.0\n",
    "    mean_bert_vs_c_qj = 0.0\n",
    "    mean_bert_vs_qj_aj = 0.0\n",
    "    mean_simcse_vs_c_aj = 0.0\n",
    "    mean_simcse_vs_c_qj = 0.0\n",
    "    mean_simcse_vs_qj_aj = 0.0\n",
    "    eval_data_size = 0\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    c_aj = [context, answers_joint]\n",
    "                    ngram_vs_c_aj = text_utils.ngram_vendi_score(sents=c_aj, ns=[1, 2])\n",
    "                    bert_vs_c_aj = text_utils.embedding_vendi_score(\n",
    "                        sents=c_aj, model_path=\"bert-base-uncased\"\n",
    "                    )\n",
    "                    simcse_vs_c_aj = text_utils.embedding_vendi_score(\n",
    "                        sents=c_aj,\n",
    "                        model_path=\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "                    )\n",
    "\n",
    "                    c_qj = [context, queries_joint]\n",
    "                    ngram_vs_c_qj = text_utils.ngram_vendi_score(sents=c_qj, ns=[1, 2])\n",
    "                    bert_vs_c_qj = text_utils.embedding_vendi_score(\n",
    "                        sents=c_qj, model_path=\"bert-base-uncased\"\n",
    "                    )\n",
    "                    simcse_vs_c_qj = text_utils.embedding_vendi_score(\n",
    "                        sents=c_qj,\n",
    "                        model_path=\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "                    )\n",
    "\n",
    "                    qj_aj = [queries_joint, answers_joint]\n",
    "                    ngram_vs_qj_aj = text_utils.ngram_vendi_score(\n",
    "                        sents=qj_aj, ns=[1, 2]\n",
    "                    )\n",
    "                    bert_vs_qj_aj = text_utils.embedding_vendi_score(\n",
    "                        sents=qj_aj, model_path=\"bert-base-uncased\"\n",
    "                    )\n",
    "                    simcse_vs_qj_aj = text_utils.embedding_vendi_score(\n",
    "                        sents=qj_aj,\n",
    "                        model_path=\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "                    )\n",
    "\n",
    "                    mean_ngram_vs_c_aj = mean_ngram_vs_c_aj + ngram_vs_c_aj\n",
    "                    mean_ngram_vs_c_qj = mean_ngram_vs_c_qj + ngram_vs_c_qj\n",
    "                    mean_ngram_vs_qj_aj = mean_ngram_vs_qj_aj + ngram_vs_qj_aj\n",
    "                    mean_bert_vs_c_aj = mean_bert_vs_c_aj + bert_vs_c_aj\n",
    "                    mean_bert_vs_c_qj = mean_bert_vs_c_qj + bert_vs_c_qj\n",
    "                    mean_bert_vs_qj_aj = mean_bert_vs_qj_aj + bert_vs_qj_aj\n",
    "                    mean_simcse_vs_c_aj = mean_simcse_vs_c_aj + simcse_vs_c_aj\n",
    "                    mean_simcse_vs_c_qj = mean_simcse_vs_c_qj + simcse_vs_c_qj\n",
    "                    mean_simcse_vs_qj_aj = mean_simcse_vs_qj_aj + simcse_vs_qj_aj\n",
    "\n",
    "    return (\n",
    "        eval_data_size,\n",
    "        (\n",
    "            mean_ngram_vs_c_aj / eval_data_size,\n",
    "            mean_ngram_vs_c_qj / eval_data_size,\n",
    "            mean_ngram_vs_qj_aj / eval_data_size,\n",
    "        ),\n",
    "        (\n",
    "            mean_bert_vs_c_aj / eval_data_size,\n",
    "            mean_bert_vs_c_qj / eval_data_size,\n",
    "            mean_bert_vs_qj_aj / eval_data_size,\n",
    "        ),\n",
    "        (\n",
    "            mean_simcse_vs_c_aj / eval_data_size,\n",
    "            mean_simcse_vs_c_qj / eval_data_size,\n",
    "            mean_simcse_vs_qj_aj / eval_data_size,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687762ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:06:35.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_VENDIscores\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n",
      "\u001b[32m2024-06-16 16:06:43.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_VENDIscores\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n",
      "\u001b[32m2024-06-16 16:06:49.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_VENDIscores\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n",
      "\u001b[32m2024-06-16 16:06:55.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_VENDIscores\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " (1.8516522838990441, 1.9252092854489706, 1.8188529255221262),\n",
       " (1.2015637457370758, 1.1242541074752808, 1.1333279013633728),\n",
       " (1.5365206003189087, 1.5628820657730103, 1.3434661626815796))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "get_average_VENDIscores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e8b27",
   "metadata": {},
   "source": [
    "## ROUGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb399e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "def get_average_RougeScores(data):\n",
    "    mean_rougeL_qj_c = [{\"rouge-l\": {\"r\": 0.0, \"p\": 0.0, \"f\": 0.0}}]\n",
    "    mean_rougeL_aj_c = [{\"rouge-l\": {\"r\": 0.0, \"p\": 0.0, \"f\": 0.0}}]\n",
    "    mean_rougeL_qj_aj = [{\"rouge-l\": {\"r\": 0.0, \"p\": 0.0, \"f\": 0.0}}]\n",
    "\n",
    "    eval_data_size = 0\n",
    "    rouge_evaluator = Rouge(metrics=[\"rouge-l\"])\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    # [{'rouge-l': {'r': 0.2235294117647059, 'p': 0.6129032258064516, 'f': 0.3275862029800833}}]\n",
    "                    rougeL_qj_c = rouge_evaluator.get_scores(\n",
    "                        hyps=queries_joint, refs=context\n",
    "                    )[0]\n",
    "                    rougeL_aj_c = rouge_evaluator.get_scores(\n",
    "                        hyps=answers_joint, refs=context\n",
    "                    )[0]\n",
    "                    rougeL_qj_aj = rouge_evaluator.get_scores(\n",
    "                        hyps=queries_joint, refs=answers_joint\n",
    "                    )[0]\n",
    "\n",
    "                    mean_rougeL_qj_c[0][\"rouge-l\"][\"r\"] = (\n",
    "                        mean_rougeL_qj_c[0][\"rouge-l\"][\"r\"]\n",
    "                        + rougeL_qj_c[\"rouge-l\"][\"r\"]\n",
    "                    )\n",
    "                    mean_rougeL_qj_c[0][\"rouge-l\"][\"p\"] = (\n",
    "                        mean_rougeL_qj_c[0][\"rouge-l\"][\"p\"]\n",
    "                        + rougeL_qj_c[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "                    mean_rougeL_qj_c[0][\"rouge-l\"][\"f\"] = (\n",
    "                        mean_rougeL_qj_c[0][\"rouge-l\"][\"f\"]\n",
    "                        + rougeL_qj_c[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "\n",
    "                    mean_rougeL_aj_c[0][\"rouge-l\"][\"r\"] = (\n",
    "                        mean_rougeL_aj_c[0][\"rouge-l\"][\"r\"]\n",
    "                        + rougeL_aj_c[\"rouge-l\"][\"r\"]\n",
    "                    )\n",
    "                    mean_rougeL_aj_c[0][\"rouge-l\"][\"p\"] = (\n",
    "                        mean_rougeL_aj_c[0][\"rouge-l\"][\"p\"]\n",
    "                        + rougeL_aj_c[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "                    mean_rougeL_aj_c[0][\"rouge-l\"][\"f\"] = (\n",
    "                        mean_rougeL_aj_c[0][\"rouge-l\"][\"f\"]\n",
    "                        + rougeL_aj_c[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "\n",
    "                    mean_rougeL_qj_aj[0][\"rouge-l\"][\"r\"] = (\n",
    "                        mean_rougeL_qj_aj[0][\"rouge-l\"][\"r\"]\n",
    "                        + rougeL_qj_aj[\"rouge-l\"][\"r\"]\n",
    "                    )\n",
    "                    mean_rougeL_qj_aj[0][\"rouge-l\"][\"p\"] = (\n",
    "                        mean_rougeL_qj_aj[0][\"rouge-l\"][\"p\"]\n",
    "                        + rougeL_qj_aj[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "                    mean_rougeL_qj_aj[0][\"rouge-l\"][\"f\"] = (\n",
    "                        mean_rougeL_qj_aj[0][\"rouge-l\"][\"f\"]\n",
    "                        + rougeL_qj_aj[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "\n",
    "    mean_rougeL_qj_c[0][\"rouge-l\"][\"r\"] = (\n",
    "        mean_rougeL_qj_c[0][\"rouge-l\"][\"r\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_qj_c[0][\"rouge-l\"][\"p\"] = (\n",
    "        mean_rougeL_qj_c[0][\"rouge-l\"][\"p\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_qj_c[0][\"rouge-l\"][\"f\"] = (\n",
    "        mean_rougeL_qj_c[0][\"rouge-l\"][\"f\"] / eval_data_size\n",
    "    )\n",
    "\n",
    "    mean_rougeL_aj_c[0][\"rouge-l\"][\"r\"] = (\n",
    "        mean_rougeL_aj_c[0][\"rouge-l\"][\"r\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_aj_c[0][\"rouge-l\"][\"p\"] = (\n",
    "        mean_rougeL_aj_c[0][\"rouge-l\"][\"p\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_aj_c[0][\"rouge-l\"][\"f\"] = (\n",
    "        mean_rougeL_aj_c[0][\"rouge-l\"][\"f\"] / eval_data_size\n",
    "    )\n",
    "\n",
    "    mean_rougeL_qj_aj[0][\"rouge-l\"][\"r\"] = (\n",
    "        mean_rougeL_qj_aj[0][\"rouge-l\"][\"r\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_qj_aj[0][\"rouge-l\"][\"p\"] = (\n",
    "        mean_rougeL_qj_aj[0][\"rouge-l\"][\"p\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_qj_aj[0][\"rouge-l\"][\"f\"] = (\n",
    "        mean_rougeL_qj_aj[0][\"rouge-l\"][\"f\"] / eval_data_size\n",
    "    )\n",
    "\n",
    "    return (eval_data_size, mean_rougeL_qj_c, mean_rougeL_aj_c, mean_rougeL_qj_aj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8086f4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:07:09.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_RougeScores\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n",
      "\u001b[32m2024-06-16 16:07:09.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_RougeScores\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:07:09.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_RougeScores\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n",
      "\u001b[32m2024-06-16 16:07:09.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_RougeScores\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " [{'rouge-l': {'r': 0.30518501243781093,\n",
       "    'p': 0.33564351293458833,\n",
       "    'f': 0.33564351293458833}}],\n",
       " [{'rouge-l': {'r': 0.4674945584577115,\n",
       "    'p': 0.305294510917761,\n",
       "    'f': 0.305294510917761}}],\n",
       " [{'rouge-l': {'r': 0.22650492283431187,\n",
       "    'p': 0.3251130311122468,\n",
       "    'f': 0.3251130311122468}}])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_RougeScores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0afab",
   "metadata": {},
   "source": [
    "## BLEU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf144a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu, bleu_score\n",
    "\n",
    "\n",
    "def get_average_BlueScores(data):\n",
    "    mean_bleu_aj_c = 0.0\n",
    "    mean_bleu_qj_c = 0.0\n",
    "    mean_bleu_qj_aj = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    bleu_aj_c = bleu(\n",
    "                        references=[context.split()],\n",
    "                        hypothesis=answers_joint.split(),\n",
    "                        weights=(1,),\n",
    "                    )\n",
    "                    bleu_qj_c = bleu(\n",
    "                        references=[context.split()],\n",
    "                        hypothesis=queries_joint.split(),\n",
    "                        weights=(1,),\n",
    "                    )\n",
    "                    bleu_qj_aj = bleu(\n",
    "                        references=[answers_joint.split()],\n",
    "                        hypothesis=queries_joint.split(),\n",
    "                        weights=(1,),\n",
    "                    )\n",
    "\n",
    "                    mean_bleu_aj_c = mean_bleu_aj_c + bleu_aj_c\n",
    "                    mean_bleu_qj_c = mean_bleu_qj_c + bleu_qj_c\n",
    "                    mean_bleu_qj_aj = mean_bleu_qj_aj + bleu_qj_aj\n",
    "\n",
    "    mean_bleu_aj_c = mean_bleu_aj_c / eval_data_size\n",
    "    mean_bleu_qj_c = mean_bleu_qj_c / eval_data_size\n",
    "    mean_bleu_qj_aj = mean_bleu_qj_aj / eval_data_size\n",
    "\n",
    "    return (eval_data_size, (mean_bleu_aj_c, mean_bleu_qj_c, mean_bleu_qj_aj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99166a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:07:51.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_BlueScores\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n",
      "\u001b[32m2024-06-16 16:07:51.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_BlueScores\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n",
      "\u001b[32m2024-06-16 16:07:51.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_BlueScores\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n",
      "\u001b[32m2024-06-16 16:07:51.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_BlueScores\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, (0.14838540377630685, 0.21865774528063278, 0.17381451091717182))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_BlueScores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11501069",
   "metadata": {},
   "source": [
    "## METEOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d677c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "def get_average_MeteorScores(data):\n",
    "\n",
    "    mean_meteor_q_a = 0.0\n",
    "    mean_meteor_q_c = 0.0\n",
    "    mean_meteor_a_c = 0.0\n",
    "    mean_meteor_aj_c = 0.0\n",
    "    mean_meteor_qj_c = 0.0\n",
    "    mean_meteor_qj_aj = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    meteor_aj_c = meteor(\n",
    "                        references=[word_tokenize(context)],\n",
    "                        hypothesis=word_tokenize(answers_joint),\n",
    "                    )\n",
    "                    meteor_qj_c = meteor(\n",
    "                        references=[word_tokenize(context)],\n",
    "                        hypothesis=word_tokenize(queries_joint),\n",
    "                    )\n",
    "                    meteor_qj_aj = meteor(\n",
    "                        references=[word_tokenize(answers_joint)],\n",
    "                        hypothesis=word_tokenize(queries_joint),\n",
    "                    )\n",
    "\n",
    "                    mean_meteor_aj_c = mean_meteor_aj_c + meteor_aj_c\n",
    "                    mean_meteor_qj_c = mean_meteor_qj_c + meteor_qj_c\n",
    "                    mean_meteor_qj_aj = mean_meteor_qj_aj + meteor_qj_aj\n",
    "\n",
    "                    mean_a_q = 0.0\n",
    "                    mean_c_q = 0.0\n",
    "                    mean_c_a = 0.0\n",
    "                    for q, a in zip(queries, answers):\n",
    "                        a_q = meteor(\n",
    "                            references=[word_tokenize(q)], hypothesis=word_tokenize(a)\n",
    "                        )\n",
    "                        c_q = meteor(\n",
    "                            references=[word_tokenize(context)],\n",
    "                            hypothesis=word_tokenize(q),\n",
    "                        )\n",
    "                        c_a = meteor(\n",
    "                            references=[word_tokenize(context)],\n",
    "                            hypothesis=word_tokenize(a),\n",
    "                        )\n",
    "                        mean_a_q = mean_a_q + a_q\n",
    "                        mean_c_q = mean_c_q + c_q\n",
    "                        mean_c_a = mean_c_a + c_a\n",
    "\n",
    "                    mean_a_q = mean_a_q / len(queries)\n",
    "                    mean_c_q = mean_c_q / len(queries)\n",
    "                    mean_c_a = mean_c_a / len(queries)\n",
    "\n",
    "                    mean_meteor_q_a = mean_meteor_q_a + mean_a_q\n",
    "                    mean_meteor_q_c = mean_meteor_q_c + mean_c_q\n",
    "                    mean_meteor_a_c = mean_meteor_a_c + mean_c_a\n",
    "\n",
    "    mean_meteor_q_a = mean_meteor_q_a / eval_data_size\n",
    "    mean_meteor_q_c = mean_meteor_q_c / eval_data_size\n",
    "    mean_meteor_a_c = mean_meteor_a_c / eval_data_size\n",
    "\n",
    "    mean_meteor_aj_c = mean_meteor_aj_c / eval_data_size\n",
    "    mean_meteor_qj_c = mean_meteor_qj_c / eval_data_size\n",
    "    mean_meteor_qj_aj = mean_meteor_qj_aj / eval_data_size\n",
    "\n",
    "    return (\n",
    "        eval_data_size,\n",
    "        (mean_meteor_a_c, mean_meteor_q_c, mean_meteor_q_a),\n",
    "        (mean_meteor_aj_c, mean_meteor_qj_c, mean_meteor_qj_aj),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcd64a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:08:31.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_MeteorScores\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n",
      "\u001b[32m2024-06-16 16:08:32.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_MeteorScores\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n",
      "\u001b[32m2024-06-16 16:08:32.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_MeteorScores\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n",
      "\u001b[32m2024-06-16 16:08:32.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_MeteorScores\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " (0.12240957033556074, 0.0634707476407487, 0.4848708153735204),\n",
       " (0.32550517905570275, 0.2470300445717199, 0.2597391301011615))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_MeteorScores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d7167",
   "metadata": {},
   "source": [
    "## Jaccard Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d04a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(ref, hyp):\n",
    "    \"\"\"returns the jaccard similarity between two lists\"\"\"\n",
    "    intersection_cardinality = len(set.intersection(*[set(ref), set(hyp)]))\n",
    "    union_cardinality = len(set.union(*[set(ref), set(hyp)]))\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "\n",
    "def get_average_JaccardScores(data):\n",
    "    mean_aj_c = 0.0\n",
    "    mean_qj_c = 0.0\n",
    "    mean_qj_aj = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    aj_c = jaccard_similarity(context, answers_joint)\n",
    "                    qj_c = jaccard_similarity(context, queries_joint)\n",
    "                    qj_aj = jaccard_similarity(answers_joint, queries_joint)\n",
    "\n",
    "                    mean_aj_c = mean_aj_c + aj_c\n",
    "                    mean_qj_c = mean_qj_c + qj_c\n",
    "                    mean_qj_aj = mean_qj_aj + qj_aj\n",
    "\n",
    "    mean_aj_c = mean_aj_c / eval_data_size\n",
    "    mean_qj_c = mean_qj_c / eval_data_size\n",
    "    mean_qj_aj = mean_qj_aj / eval_data_size\n",
    "\n",
    "    return (eval_data_size, (mean_aj_c, mean_qj_c, mean_qj_aj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c0f2c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:08:58.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_JaccardScores\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n",
      "\u001b[32m2024-06-16 16:08:58.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_JaccardScores\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n",
      "\u001b[32m2024-06-16 16:08:58.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_JaccardScores\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n",
      "\u001b[32m2024-06-16 16:08:58.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_JaccardScores\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, (0.7550061418929949, 0.7305450677543701, 0.7669600506195431))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_JaccardScores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7653b",
   "metadata": {},
   "source": [
    "## Distinct-n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "216ba661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimw.app.services.eval.distinct_n import (\n",
    "    distinct_n_sentence_level,\n",
    "    distinct_n_corpus_level,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "def jaccard_similarity(ref, hyp):\n",
    "\n",
    "    \"\"\"returns the jaccard similarity between two lists\"\"\"\n",
    "\n",
    "    intersection_cardinality = len(set.intersection(*[set(ref), set(hyp)]))\n",
    "\n",
    "    union_cardinality = len(set.union(*[set(ref), set(hyp)]))\n",
    "\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_average_DistinctNScores(data):\n",
    "\n",
    "    mean_a_dist1 = 0.0\n",
    "    mean_q_dist1 = 0.0\n",
    "    mean_a_dist2 = 0.0\n",
    "    mean_q_dist2 = 0.0\n",
    "    mean_aj_dist1 = 0.0\n",
    "    mean_qj_dist1 = 0.0\n",
    "    mean_aj_dist2 = 0.0\n",
    "    mean_qj_dist2 = 0.0\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "\n",
    "                    # Extract targets\n",
    "\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "\n",
    "                    context = doc[\"doc\"]\n",
    "\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    a_dist1 = distinct_n_corpus_level(answers, 1)\n",
    "                    q_dist1 = distinct_n_corpus_level(queries, 1)\n",
    "\n",
    "                    a_dist2 = distinct_n_corpus_level(answers, 2)\n",
    "                    q_dist2 = distinct_n_corpus_level(queries, 2)\n",
    "\n",
    "                    aj_dist1 = distinct_n_sentence_level(answers_joint, 1)\n",
    "                    qj_dist1 = distinct_n_sentence_level(queries_joint, 1)\n",
    "\n",
    "                    aj_dist2 = distinct_n_sentence_level(answers_joint, 2)\n",
    "                    qj_dist2 = distinct_n_sentence_level(queries_joint, 2)\n",
    "\n",
    "                    mean_a_dist1 = mean_a_dist1 + a_dist1\n",
    "                    mean_q_dist1 = mean_q_dist1 + q_dist1\n",
    "                    mean_a_dist2 = mean_a_dist2 + a_dist2\n",
    "                    mean_q_dist2 = mean_q_dist2 + q_dist2\n",
    "                    mean_aj_dist1 = mean_aj_dist1 + aj_dist1\n",
    "                    mean_qj_dist1 = mean_qj_dist1 + qj_dist1\n",
    "                    mean_aj_dist2 = mean_aj_dist2 + aj_dist2\n",
    "                    mean_qj_dist2 = mean_qj_dist2 + qj_dist2\n",
    "\n",
    "    mean_a_dist1 = mean_a_dist1 / eval_data_size\n",
    "    mean_q_dist1 = mean_q_dist1 / eval_data_size\n",
    "    mean_a_dist2 = mean_a_dist2 / eval_data_size\n",
    "    mean_q_dist2 = mean_q_dist2 / eval_data_size\n",
    "    mean_aj_dist1 = mean_aj_dist1 / eval_data_size\n",
    "    mean_qj_dist1 = mean_qj_dist1 / eval_data_size\n",
    "    mean_aj_dist2 = mean_aj_dist2 / eval_data_size\n",
    "    mean_qj_dist2 = mean_qj_dist2 / eval_data_size\n",
    "\n",
    "    return (\n",
    "        eval_data_size,\n",
    "        (\n",
    "            mean_a_dist1,\n",
    "            mean_q_dist1,\n",
    "            mean_a_dist2,\n",
    "            mean_q_dist2,\n",
    "            mean_aj_dist1,\n",
    "            mean_qj_dist1,\n",
    "            mean_aj_dist2,\n",
    "            mean_qj_dist2,\n",
    "        ),\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba086b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:20:47.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_DistinctNScores\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n",
      "\u001b[32m2024-06-16 16:20:47.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_DistinctNScores\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n",
      "\u001b[32m2024-06-16 16:20:47.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_DistinctNScores\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n",
      "\u001b[32m2024-06-16 16:20:47.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_DistinctNScores\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " (0.18682549955800978,\n",
       "  0.36802365441429297,\n",
       "  0.6644687015747955,\n",
       "  0.8235171374143389,\n",
       "  0.023633874870904632,\n",
       "  0.04994832337031325,\n",
       "  0.1527758809227636,\n",
       "  0.23300355527246447))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_DistinctNScores = get_average_DistinctNScores(data)\n",
    "average_DistinctNScores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac5fe1",
   "metadata": {},
   "source": [
    "## G-Eval / DeepEval - Hallucinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e9f34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from aimw.app.core.ai_config import get_ai_settings\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    get_ai_settings().openai_api_key\n",
    ")\n",
    "\n",
    "os.environ[\"DEEPEVAL_RESULTS_FOLDER\"] = \"./output/deepeval/hallucination\"\n",
    "\n",
    "# DeepEval Available GPT models: gpt-4o, gpt-4-turbo, gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4, gpt-4-32k, gpt-4-0613, gpt-4-32k-0613, gpt-3.5-turbo-1106, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125\n",
    "model_name = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c76b318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "def get_average_HallucinationScores(data):\n",
    "    mean_score = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    test_case = LLMTestCase(\n",
    "                        input=queries_joint,\n",
    "                        actual_output=answers_joint,\n",
    "                        context=[context],\n",
    "                    )\n",
    "                    metric = HallucinationMetric(threshold=0.5, model=model_name)\n",
    "\n",
    "                    metric.measure(test_case)\n",
    "\n",
    "                    logger.info(metric.score)\n",
    "                    logger.info(metric.reason)\n",
    "                    logger.info\n",
    "\n",
    "                    mean_score = mean_score + metric.score\n",
    "                    # or evaluate test cases in bulk\n",
    "                    # evaluate([test_case], [metric])\n",
    "\n",
    "    mean_score = mean_score / eval_data_size\n",
    "\n",
    "    return (eval_data_size, mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e856df3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:14:05.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0321f767d1a4abcb46e36b424bb66f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:14:09.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m1.0\u001b[0m\n",
      "\u001b[32m2024-06-16 16:14:09.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mThe hallucination score is 1.00 because the actual output fails to address the key context related to the primary responsibility of software companies and the importance of acquiring marketable skills for students, indicating a complete lack of relevance and accuracy in addressing the given information.\u001b[0m\n",
      "\u001b[32m2024-06-16 16:14:09.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dd1d06c8cf4440b172b7c706f9b26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:14:13.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m1.0\u001b[0m\n",
      "\u001b[32m2024-06-16 16:14:13.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mThe hallucination score is 1.00 because the actual output aligns with the context provided, offering additional information without any contradictions.\u001b[0m\n",
      "\u001b[32m2024-06-16 16:14:13.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584fc15122c04cf78470611efd3b2f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:14:17.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m0.0\u001b[0m\n",
      "\u001b[32m2024-06-16 16:14:17.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mThe hallucination score is 0.00 because the actual output aligns perfectly with the context provided, indicating there is no discrepancy or false information present.\u001b[0m\n",
      "\u001b[32m2024-06-16 16:14:17.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df63640c8e047b2800017b0e0cf015f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:14:21.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m0.0\u001b[0m\n",
      "\u001b[32m2024-06-16 16:14:21.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_HallucinationScores\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mThe score is 0.00 because the actual output perfectly aligns with the context provided, indicating no hallucinations or inaccuracies in the generated text.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 0.5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_HallucinationScores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ae07e",
   "metadata": {},
   "source": [
    "## DeepEval Contextual Relevancy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "252c855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "def get_average_ContextualRelevancyScores(data):\n",
    "    mean_score = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    metric = ContextualRelevancyMetric(\n",
    "                        threshold=0.7, model=model_name, include_reason=True\n",
    "                    )\n",
    "                    test_case = LLMTestCase(\n",
    "                        input=queries_joint,\n",
    "                        actual_output=answers_joint,\n",
    "                        retrieval_context=[context],\n",
    "                    )\n",
    "\n",
    "                    metric.measure(test_case)\n",
    "                    logger.info(metric.score)\n",
    "                    logger.info(metric.reason)\n",
    "\n",
    "                    mean_score = mean_score + metric.score\n",
    "                    # or evaluate test cases in bulk\n",
    "                    # evaluate([test_case], [metric])\n",
    "\n",
    "    mean_score = mean_score / eval_data_size\n",
    "\n",
    "    return (eval_data_size, mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83029114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:17:29.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561ca14c9da7424a87fbe199c2c52ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:17:32.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1m1.0\u001b[0m\n",
      "\u001b[32m2024-06-16 16:17:32.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mThe score is 1.00 because the input is focused and directly addresses questions related to the responsibilities of software companies, the importance of acquiring marketable skills, and the impact of education on the job market. It maintains a consistent theme throughout, making the retrieval context entirely relevant.\u001b[0m\n",
      "\u001b[32m2024-06-16 16:17:32.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf97265dae644e7b9f179285708faed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:17:36.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1m1.0\u001b[0m\n",
      "\u001b[32m2024-06-16 16:17:36.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mThe score is 1.00 because the input addresses a wide range of questions related to credit ratings and their impact on financial institutions, market, and regulations, showing a thorough understanding of the topic.\u001b[0m\n",
      "\u001b[32m2024-06-16 16:17:36.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3029d59170f642ac96f065ea87c39bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:17:39.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1m1.0\u001b[0m\n",
      "\u001b[32m2024-06-16 16:17:39.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mThe score is 1.00 because the input contains a comprehensive list of questions related to Health FSAs, individual health insurance premiums, 125 cafeteria plans, and regulations, demonstrating a high level of relevance and specificity.\u001b[0m\n",
      "\u001b[32m2024-06-16 16:17:39.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7448189ea44baf8bf34cbb5ed23e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:17:42.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1m1.0\u001b[0m\n",
      "\u001b[32m2024-06-16 16:17:42.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_ContextualRelevancyScores\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mThe score is 1.00 because the input is focused solely on questions related to Samsung's role, dominance, and significance in the flat screen industry, making the retrieval context completely relevant. Great job!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 1.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualRelevancyScores = get_average_ContextualRelevancyScores(data)\n",
    "contextualRelevancyScores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab113120",
   "metadata": {},
   "source": [
    "## DeepEval Faithfulness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9e9c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    HallucinationMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "def get_average_FaithfulnessMetric(data):\n",
    "    mean_score = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        # if doc[\"docid\"] == 313017:\n",
    "        #     logger.info(doc)\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    metric = FaithfulnessMetric(\n",
    "                        threshold=0.7, model=model_name, include_reason=True\n",
    "                    )\n",
    "                    test_case = LLMTestCase(\n",
    "                        input=queries_joint,\n",
    "                        actual_output=answers_joint,\n",
    "                        retrieval_context=[context],\n",
    "                    )\n",
    "\n",
    "                    metric.measure(test_case)\n",
    "                    logger.info(metric.score)\n",
    "                    logger.info(metric.reason)\n",
    "\n",
    "                    mean_score = mean_score + metric.score\n",
    "                    # or evaluate test cases in bulk\n",
    "                    # evaluate([test_case], [metric])\n",
    "\n",
    "    mean_score = mean_score / eval_data_size\n",
    "\n",
    "    return (eval_data_size, mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9eb51d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:18:24.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mdocid: 3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625c84305d9046c4a66376d482382499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:18:34.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m0.7777777777777778\u001b[0m\n",
      "\u001b[32m2024-06-16 16:18:34.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mThe score is 0.78 because the actual output presents contradictions by claiming that the company does provide on-the-job training, which goes against the information in the retrieval context that states training workers is not the company's job and software companies may not provide on-the-job training.\u001b[0m\n",
      "\u001b[32m2024-06-16 16:18:34.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mdocid: 31\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3230604072b4a98be4832b233f255c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:18:46.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m0.875\u001b[0m\n",
      "\u001b[32m2024-06-16 16:18:46.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mThe score is 0.88 because the actual output accurately acknowledges the contradiction that new controls require financial institutions to conduct their own due diligence, rather than solely relying on credit ratings. This shows a high level of faithfulness to the retrieval context.\u001b[0m\n",
      "\u001b[32m2024-06-16 16:18:46.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mdocid: 56\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2221dd772474cb7bf7c6e18386628ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:18:55.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m0.875\u001b[0m\n",
      "\u001b[32m2024-06-16 16:18:55.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mThe score is 0.88 because the claim in the actual output contradicts the retrieval context by stating that using a cafeteria plan to pay for individual premiums is allowed under N. 2013-54, when in fact it is effectively prohibited.\u001b[0m\n",
      "\u001b[32m2024-06-16 16:18:55.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mdocid: 59\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65001dcda9244cadac790e89d2c353d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-16 16:19:03.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m1.0\u001b[0m\n",
      "\u001b[32m2024-06-16 16:19:03.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_average_FaithfulnessMetric\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mThe score is 1.00 because there are no contradictions present, indicating a high level of faithfulness in the actual output to the retrieval context. Great job!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 0.8819444444444444)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulnessMetric = get_average_FaithfulnessMetric(data)\n",
    "faithfulnessMetric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
