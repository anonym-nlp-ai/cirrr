{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cfc6fc",
   "metadata": {},
   "source": [
    "# Automatic Evaluation on a few samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c387652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd to \"Directory of \"./alignment\"\n",
    "%cd ../../\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9a6d2-29f2-434d-906c-ecba3839b6ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !conda activate phd-align-py3-11\n",
    "import torch\n",
    "\n",
    "logger.info(torch.cuda.is_available())\n",
    "logger.info(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e616a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\n",
    "    \"docid\": 54251,\n",
    "    \"doc\": \"There are two types of credit checks. First is the hard pull which is typically done when you apply for a credit line. The lender will hard pull your file and make his/her decision based on that. This affects your score negatively. You might lose few points for one hard inquiry. Second type is soft pull, which is done as a background check. Typically done by credit card companies to send you a pre-approved offer, or renting an apartment etc. This does not affect your score. One thing to keep in mind is a company will not do a hard pull without your permission, where as they can do soft pulls without you even knowing.  Soft inquiries vs hard inquiries\",\n",
    "    \"tokenized_size\": 140.0,\n",
    "    \"max_seq_len_exceeded\": False,\n",
    "    \"cct_saar\": {\n",
    "        \"queries_aspects\": [\n",
    "            {\n",
    "                \"question\": \"What are the two types of credit checks?\",\n",
    "                \"answer\": \"There are two types of credit checks: hard pull and soft pull. A hard pull is typically done when you apply for a credit line, while a soft pull is done as a background check.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"How does a hard pull affect your credit score?\",\n",
    "                \"answer\": \"A hard pull can negatively affect your credit score, causing you to lose a few points.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the difference between a hard pull and a soft pull?\",\n",
    "                \"answer\": \"A hard pull is a more invasive check that affects your credit score, while a soft pull is a background check that does not affect your score.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Can a company do a hard pull without your permission?\",\n",
    "                \"answer\": \"No, a company cannot do a hard pull without your permission.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the purpose of a soft pull?\",\n",
    "                \"answer\": \"A soft pull is typically done as a background check, such as when a credit card company sends you a pre-approved offer or when you're renting an apartment.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the main difference between a hard pull and a soft pull?\",\n",
    "                \"answer\": \"The main difference is that a hard pull affects your credit score, while a soft pull does not.\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the impact of a hard pull on your credit score?\",\n",
    "                \"answer\": \"A hard pull can cause a temporary decrease in your credit score, but the impact is usually minor.\",\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimw.app.utils.json_utils import JSONProcessor\n",
    "\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dir = \"./re/data/fiqa/clean/base/exp_001/\"\n",
    "w_dir = \"\"\n",
    "jSONProcessor = JSONProcessor(r_dir + \"cct_saar_corpus_cln_split_0.json\")\n",
    "data = jSONProcessor.read_json_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eafb2e",
   "metadata": {},
   "source": [
    "## SentenceTransformer Using `BAAI/bge-large-en-v1.5`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3074ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "\n",
    "def get_average_scores(data):\n",
    "    mean_of_mean_score_context_vs_queries = 0.0\n",
    "    mean_of_mean_score_context_vs_queries_joint = 0.0\n",
    "    mean_of_mean_score_context_vs_answers = 0.0\n",
    "    mean_of_mean_score_context_vs_answers_joint = 0.0\n",
    "    mean_of_mean_score_questions_answers = 0.0\n",
    "    mean_of_mean_score_questions_joint_vs_answers_joint = 0.0\n",
    "    mean_of_mean_score_questions_joint_and_answers_joint_vs_context = 0.0\n",
    "    mean_of_mean_score_questions_and_answers_vs_context = 0.0\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    number_of_pairs = len(answers)\n",
    "                    # Embed targets\n",
    "                    context_embedding = model.encode(context, normalize_embeddings=True)\n",
    "                    queries_embeddings = model.encode(\n",
    "                        queries, normalize_embeddings=True\n",
    "                    )\n",
    "                    queries_joint_embeddings = model.encode(\n",
    "                        queries_joint, normalize_embeddings=True\n",
    "                    )\n",
    "                    answers_embeddings = model.encode(\n",
    "                        answers, normalize_embeddings=True\n",
    "                    )\n",
    "                    answers_joint_embeddings = model.encode(\n",
    "                        answers_joint, normalize_embeddings=True\n",
    "                    )\n",
    "\n",
    "                    # Compute similarity scores\n",
    "\n",
    "                    ############# context Vs queries #############\n",
    "                    similarity_context_queries = (\n",
    "                        context_embedding @ queries_embeddings.T\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarity_context_queries: {similarity_context_queries}\"\n",
    "                    )\n",
    "                    mean_score_context_vs_queries = sum(\n",
    "                        similarity_context_queries\n",
    "                    ) / len(\n",
    "                        similarity_context_queries\n",
    "                    )  # <----------\n",
    "                    mean_of_mean_score_context_vs_queries = (\n",
    "                        mean_of_mean_score_context_vs_queries\n",
    "                        + mean_score_context_vs_queries\n",
    "                    )\n",
    "\n",
    "                    ############# context Vs concatenated queries #############\n",
    "                    similarity_context_queries_joint = (\n",
    "                        context_embedding @ queries_joint_embeddings.T\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarity_context_queries_joint: {similarity_context_queries_joint}\"\n",
    "                    )\n",
    "                    mean_of_mean_score_context_vs_queries_joint = (\n",
    "                        mean_of_mean_score_context_vs_queries_joint\n",
    "                        + similarity_context_queries_joint\n",
    "                    )\n",
    "\n",
    "                    ############# context Vs answers #############\n",
    "                    similarity_context_answers = (\n",
    "                        context_embedding @ answers_embeddings.T\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarity_context_answers: {similarity_context_answers}\"\n",
    "                    )\n",
    "                    mean_score_context_vs_answers = sum(\n",
    "                        similarity_context_answers\n",
    "                    ) / len(\n",
    "                        similarity_context_answers\n",
    "                    )  # <----------\n",
    "                    mean_of_mean_score_context_vs_answers = (\n",
    "                        mean_of_mean_score_context_vs_answers\n",
    "                        + mean_score_context_vs_answers\n",
    "                    )\n",
    "                    ############# context Vs concatenated answers #############\n",
    "                    similarity_context_answers_joint = (\n",
    "                        context_embedding @ answers_joint_embeddings.T\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarity_context_answers_joint: {similarity_context_answers_joint}\"\n",
    "                    )\n",
    "                    mean_of_mean_score_context_vs_answers_joint = (\n",
    "                        mean_of_mean_score_context_vs_answers_joint\n",
    "                        + similarity_context_answers_joint\n",
    "                    )\n",
    "\n",
    "                    ################ Questions Vs Answers ################\n",
    "                    similarity_questions_answers = [\n",
    "                        x @ y.T for x, y in zip(answers_embeddings, queries_embeddings)\n",
    "                    ]\n",
    "                    mean_score_questions_answers = sum(\n",
    "                        similarity_questions_answers\n",
    "                    ) / len(\n",
    "                        similarity_questions_answers\n",
    "                    )  # <-----------\n",
    "                    mean_of_mean_score_questions_answers = (\n",
    "                        mean_of_mean_score_questions_answers\n",
    "                        + mean_score_questions_answers\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarty scores: {similarity_questions_answers} | average similarity score: {mean_score_questions_answers}\"\n",
    "                    )\n",
    "\n",
    "                    ################ Questions Joint Vs Answers Joint ################\n",
    "                    mean_score_questions_joint_vs_answers_joint = (\n",
    "                        answers_joint_embeddings @ queries_joint_embeddings.T\n",
    "                    )  # <----------\n",
    "                    mean_of_mean_score_questions_joint_vs_answers_joint = (\n",
    "                        mean_of_mean_score_questions_joint_vs_answers_joint\n",
    "                        + mean_score_questions_joint_vs_answers_joint\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarty scores of concatenated Queries Vs Answers: {mean_score_questions_joint_vs_answers_joint}\"\n",
    "                    )\n",
    "\n",
    "                    ################ (Joint Questions + Joint Answers)/2 Vs Context ################\n",
    "                    mean_score_questions_joint_and_answers_joint_vs_context = (\n",
    "                        context_embedding\n",
    "                        @ ((answers_joint_embeddings + queries_joint_embeddings) / 2).T\n",
    "                    )  # <----------\n",
    "                    mean_of_mean_score_questions_joint_and_answers_joint_vs_context = (\n",
    "                        mean_of_mean_score_questions_joint_and_answers_joint_vs_context\n",
    "                        + mean_score_questions_joint_and_answers_joint_vs_context\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarty scores of concatenated Answers embeddings + concatenated Queries embeddings Vs Context: {mean_score_questions_joint_and_answers_joint_vs_context}\"\n",
    "                    )\n",
    "\n",
    "                    ################ (Questions/n + Answers/n)/2 Answers) Vs Context ################\n",
    "                    mean_answers_embeddings = sum(answers_embeddings) / len(\n",
    "                        answers_embeddings\n",
    "                    )\n",
    "                    mean_questions_embeddings = sum(queries_embeddings) / len(\n",
    "                        queries_embeddings\n",
    "                    )\n",
    "                    mean_score_questions_and_answers_vs_context = (\n",
    "                        context_embedding\n",
    "                        @ ((mean_answers_embeddings + mean_questions_embeddings) / 2).T\n",
    "                    )  # <----------\n",
    "                    mean_of_mean_score_questions_and_answers_vs_context = (\n",
    "                        mean_of_mean_score_questions_and_answers_vs_context\n",
    "                        + mean_score_questions_and_answers_vs_context\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"similarty scores of mean answers,questions embeddings Vs Context: {mean_score_questions_and_answers_vs_context}\"\n",
    "                    )\n",
    "\n",
    "    return (\n",
    "        eval_data_size,\n",
    "        (\n",
    "            mean_of_mean_score_context_vs_queries / eval_data_size,\n",
    "            mean_of_mean_score_context_vs_queries_joint / eval_data_size,\n",
    "            mean_of_mean_score_context_vs_answers / eval_data_size,\n",
    "            mean_of_mean_score_context_vs_answers_joint / eval_data_size,\n",
    "            mean_of_mean_score_questions_answers / eval_data_size,\n",
    "            mean_of_mean_score_questions_joint_vs_answers_joint / eval_data_size,\n",
    "            mean_of_mean_score_questions_joint_and_answers_joint_vs_context\n",
    "            / eval_data_size,\n",
    "            mean_of_mean_score_questions_and_answers_vs_context / eval_data_size,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# mean_of_mean_score_context_vs_queries = mean_of_mean_score_context_vs_queries / eval_data_size\n",
    "# mean_of_mean_score_context_vs_queries_joint = mean_of_mean_score_context_vs_queries_joint / eval_data_size\n",
    "# mean_of_mean_score_context_vs_answers = mean_of_mean_score_context_vs_answers / eval_data_size\n",
    "# mean_of_mean_score_context_vs_answers_joint = mean_of_mean_score_context_vs_answers_joint / eval_data_size\n",
    "# mean_of_mean_score_questions_answers = mean_of_mean_score_questions_answers / eval_data_size\n",
    "# mean_of_mean_score_questions_joint_vs_answers_joint = mean_of_mean_score_questions_joint_vs_answers_joint / eval_data_size\n",
    "# mean_of_mean_score_questions_joint_and_answers_joint_vs_context = mean_of_mean_score_questions_joint_and_answers_joint_vs_context / eval_data_size\n",
    "# mean_of_mean_score_questions_and_answers_vs_context = mean_of_mean_score_questions_and_answers_vs_context / eval_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_scores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d52a8",
   "metadata": {},
   "source": [
    "## BERTScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b57552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score\n",
    "from bert_score import score\n",
    "\n",
    "bert_score.__version__\n",
    "\n",
    "# hide the loading messages\n",
    "import logging\n",
    "import transformers\n",
    "\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def get_average_BERTscores(data):\n",
    "    mean_P_aj_c, mean_R_aj_c, mean_F1_aj_c = 0.0, 0.0, 0.0\n",
    "    mean_P_qj_c, mean_R_qj_c, mean_F1_qj_c = 0.0, 0.0, 0.0\n",
    "    mean_P_qj_aj, mean_R_qj_aj, mean_F1_qj_aj = 0.0, 0.0, 0.0\n",
    "    eval_data_size = 0\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    # Between Answers\n",
    "                    P_aj_c, R_aj_c, F1_aj_c = score(\n",
    "                        [answers_joint], [context], lang=\"en\", verbose=True\n",
    "                    )\n",
    "                    P_qj_c, R_qj_c, F1_qj_c = score(\n",
    "                        [queries_joint], [context], lang=\"en\", verbose=True\n",
    "                    )\n",
    "                    P_qj_aj, R_qj_aj, F1_qj_aj = score(\n",
    "                        [queries_joint], [answers_joint], lang=\"en\", verbose=True\n",
    "                    )\n",
    "\n",
    "                    mean_P_aj_c = mean_P_aj_c + P_aj_c\n",
    "                    mean_R_aj_c = mean_R_aj_c + R_aj_c\n",
    "                    mean_F1_aj_c = mean_F1_aj_c + F1_aj_c\n",
    "                    mean_P_qj_c = mean_P_qj_c + P_qj_c\n",
    "                    mean_R_qj_c = mean_R_qj_c + R_qj_c\n",
    "                    mean_F1_qj_c = mean_F1_qj_c + F1_qj_c\n",
    "                    mean_P_qj_aj = mean_P_qj_aj + P_qj_aj\n",
    "                    mean_R_qj_aj = mean_R_qj_aj + R_qj_aj\n",
    "                    mean_F1_qj_aj = mean_F1_qj_aj + F1_qj_aj\n",
    "\n",
    "    return (\n",
    "        eval_data_size,\n",
    "        (\n",
    "            mean_P_aj_c / eval_data_size,\n",
    "            mean_R_aj_c / eval_data_size,\n",
    "            mean_F1_aj_c / eval_data_size,\n",
    "        ),\n",
    "        (\n",
    "            mean_P_qj_c / eval_data_size,\n",
    "            mean_R_qj_c / eval_data_size,\n",
    "            mean_F1_qj_c / eval_data_size,\n",
    "        ),\n",
    "        (\n",
    "            mean_P_qj_aj / eval_data_size,\n",
    "            mean_R_qj_aj / eval_data_size,\n",
    "            mean_F1_qj_aj / eval_data_size,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_BERTscores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed486cf",
   "metadata": {},
   "source": [
    "## Vendi Score for diversity\n",
    "\n",
    "If diversity between concatned answers and the context is low (close to 1), this means comprehensivness and less hallucination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vendi_score import text_utils\n",
    "\n",
    "\n",
    "def get_average_VENDIscores(data):\n",
    "    mean_ngram_vs_c_aj = 0.0\n",
    "    mean_ngram_vs_c_qj = 0.0\n",
    "    mean_ngram_vs_qj_aj = 0.0\n",
    "    mean_bert_vs_c_aj = 0.0\n",
    "    mean_bert_vs_c_qj = 0.0\n",
    "    mean_bert_vs_qj_aj = 0.0\n",
    "    mean_simcse_vs_c_aj = 0.0\n",
    "    mean_simcse_vs_c_qj = 0.0\n",
    "    mean_simcse_vs_qj_aj = 0.0\n",
    "    eval_data_size = 0\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    c_aj = [context, answers_joint]\n",
    "                    ngram_vs_c_aj = text_utils.ngram_vendi_score(sents=c_aj, ns=[1, 2])\n",
    "                    bert_vs_c_aj = text_utils.embedding_vendi_score(\n",
    "                        sents=c_aj, model_path=\"bert-base-uncased\"\n",
    "                    )\n",
    "                    simcse_vs_c_aj = text_utils.embedding_vendi_score(\n",
    "                        sents=c_aj,\n",
    "                        model_path=\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "                    )\n",
    "\n",
    "                    c_qj = [context, queries_joint]\n",
    "                    ngram_vs_c_qj = text_utils.ngram_vendi_score(sents=c_qj, ns=[1, 2])\n",
    "                    bert_vs_c_qj = text_utils.embedding_vendi_score(\n",
    "                        sents=c_qj, model_path=\"bert-base-uncased\"\n",
    "                    )\n",
    "                    simcse_vs_c_qj = text_utils.embedding_vendi_score(\n",
    "                        sents=c_qj,\n",
    "                        model_path=\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "                    )\n",
    "\n",
    "                    qj_aj = [queries_joint, answers_joint]\n",
    "                    ngram_vs_qj_aj = text_utils.ngram_vendi_score(\n",
    "                        sents=qj_aj, ns=[1, 2]\n",
    "                    )\n",
    "                    bert_vs_qj_aj = text_utils.embedding_vendi_score(\n",
    "                        sents=qj_aj, model_path=\"bert-base-uncased\"\n",
    "                    )\n",
    "                    simcse_vs_qj_aj = text_utils.embedding_vendi_score(\n",
    "                        sents=qj_aj,\n",
    "                        model_path=\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "                    )\n",
    "\n",
    "                    mean_ngram_vs_c_aj = mean_ngram_vs_c_aj + ngram_vs_c_aj\n",
    "                    mean_ngram_vs_c_qj = mean_ngram_vs_c_qj + ngram_vs_c_qj\n",
    "                    mean_ngram_vs_qj_aj = mean_ngram_vs_qj_aj + ngram_vs_qj_aj\n",
    "                    mean_bert_vs_c_aj = mean_bert_vs_c_aj + bert_vs_c_aj\n",
    "                    mean_bert_vs_c_qj = mean_bert_vs_c_qj + bert_vs_c_qj\n",
    "                    mean_bert_vs_qj_aj = mean_bert_vs_qj_aj + bert_vs_qj_aj\n",
    "                    mean_simcse_vs_c_aj = mean_simcse_vs_c_aj + simcse_vs_c_aj\n",
    "                    mean_simcse_vs_c_qj = mean_simcse_vs_c_qj + simcse_vs_c_qj\n",
    "                    mean_simcse_vs_qj_aj = mean_simcse_vs_qj_aj + simcse_vs_qj_aj\n",
    "\n",
    "    return (\n",
    "        eval_data_size,\n",
    "        (\n",
    "            mean_ngram_vs_c_aj / eval_data_size,\n",
    "            mean_ngram_vs_c_qj / eval_data_size,\n",
    "            mean_ngram_vs_qj_aj / eval_data_size,\n",
    "        ),\n",
    "        (\n",
    "            mean_bert_vs_c_aj / eval_data_size,\n",
    "            mean_bert_vs_c_qj / eval_data_size,\n",
    "            mean_bert_vs_qj_aj / eval_data_size,\n",
    "        ),\n",
    "        (\n",
    "            mean_simcse_vs_c_aj / eval_data_size,\n",
    "            mean_simcse_vs_c_qj / eval_data_size,\n",
    "            mean_simcse_vs_qj_aj / eval_data_size,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687762ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "get_average_VENDIscores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e8b27",
   "metadata": {},
   "source": [
    "## ROUGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb399e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "def get_average_RougeScores(data):\n",
    "    mean_rougeL_qj_c = [{\"rouge-l\": {\"r\": 0.0, \"p\": 0.0, \"f\": 0.0}}]\n",
    "    mean_rougeL_aj_c = [{\"rouge-l\": {\"r\": 0.0, \"p\": 0.0, \"f\": 0.0}}]\n",
    "    mean_rougeL_qj_aj = [{\"rouge-l\": {\"r\": 0.0, \"p\": 0.0, \"f\": 0.0}}]\n",
    "\n",
    "    eval_data_size = 0\n",
    "    rouge_evaluator = Rouge(metrics=[\"rouge-l\"])\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    # [{'rouge-l': {'r': 0.2235294117647059, 'p': 0.6129032258064516, 'f': 0.3275862029800833}}]\n",
    "                    rougeL_qj_c = rouge_evaluator.get_scores(\n",
    "                        hyps=queries_joint, refs=context\n",
    "                    )[0]\n",
    "                    rougeL_aj_c = rouge_evaluator.get_scores(\n",
    "                        hyps=answers_joint, refs=context\n",
    "                    )[0]\n",
    "                    rougeL_qj_aj = rouge_evaluator.get_scores(\n",
    "                        hyps=queries_joint, refs=answers_joint\n",
    "                    )[0]\n",
    "\n",
    "                    mean_rougeL_qj_c[0][\"rouge-l\"][\"r\"] = (\n",
    "                        mean_rougeL_qj_c[0][\"rouge-l\"][\"r\"]\n",
    "                        + rougeL_qj_c[\"rouge-l\"][\"r\"]\n",
    "                    )\n",
    "                    mean_rougeL_qj_c[0][\"rouge-l\"][\"p\"] = (\n",
    "                        mean_rougeL_qj_c[0][\"rouge-l\"][\"p\"]\n",
    "                        + rougeL_qj_c[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "                    mean_rougeL_qj_c[0][\"rouge-l\"][\"f\"] = (\n",
    "                        mean_rougeL_qj_c[0][\"rouge-l\"][\"f\"]\n",
    "                        + rougeL_qj_c[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "\n",
    "                    mean_rougeL_aj_c[0][\"rouge-l\"][\"r\"] = (\n",
    "                        mean_rougeL_aj_c[0][\"rouge-l\"][\"r\"]\n",
    "                        + rougeL_aj_c[\"rouge-l\"][\"r\"]\n",
    "                    )\n",
    "                    mean_rougeL_aj_c[0][\"rouge-l\"][\"p\"] = (\n",
    "                        mean_rougeL_aj_c[0][\"rouge-l\"][\"p\"]\n",
    "                        + rougeL_aj_c[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "                    mean_rougeL_aj_c[0][\"rouge-l\"][\"f\"] = (\n",
    "                        mean_rougeL_aj_c[0][\"rouge-l\"][\"f\"]\n",
    "                        + rougeL_aj_c[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "\n",
    "                    mean_rougeL_qj_aj[0][\"rouge-l\"][\"r\"] = (\n",
    "                        mean_rougeL_qj_aj[0][\"rouge-l\"][\"r\"]\n",
    "                        + rougeL_qj_aj[\"rouge-l\"][\"r\"]\n",
    "                    )\n",
    "                    mean_rougeL_qj_aj[0][\"rouge-l\"][\"p\"] = (\n",
    "                        mean_rougeL_qj_aj[0][\"rouge-l\"][\"p\"]\n",
    "                        + rougeL_qj_aj[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "                    mean_rougeL_qj_aj[0][\"rouge-l\"][\"f\"] = (\n",
    "                        mean_rougeL_qj_aj[0][\"rouge-l\"][\"f\"]\n",
    "                        + rougeL_qj_aj[\"rouge-l\"][\"f\"]\n",
    "                    )\n",
    "\n",
    "    mean_rougeL_qj_c[0][\"rouge-l\"][\"r\"] = (\n",
    "        mean_rougeL_qj_c[0][\"rouge-l\"][\"r\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_qj_c[0][\"rouge-l\"][\"p\"] = (\n",
    "        mean_rougeL_qj_c[0][\"rouge-l\"][\"p\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_qj_c[0][\"rouge-l\"][\"f\"] = (\n",
    "        mean_rougeL_qj_c[0][\"rouge-l\"][\"f\"] / eval_data_size\n",
    "    )\n",
    "\n",
    "    mean_rougeL_aj_c[0][\"rouge-l\"][\"r\"] = (\n",
    "        mean_rougeL_aj_c[0][\"rouge-l\"][\"r\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_aj_c[0][\"rouge-l\"][\"p\"] = (\n",
    "        mean_rougeL_aj_c[0][\"rouge-l\"][\"p\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_aj_c[0][\"rouge-l\"][\"f\"] = (\n",
    "        mean_rougeL_aj_c[0][\"rouge-l\"][\"f\"] / eval_data_size\n",
    "    )\n",
    "\n",
    "    mean_rougeL_qj_aj[0][\"rouge-l\"][\"r\"] = (\n",
    "        mean_rougeL_qj_aj[0][\"rouge-l\"][\"r\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_qj_aj[0][\"rouge-l\"][\"p\"] = (\n",
    "        mean_rougeL_qj_aj[0][\"rouge-l\"][\"p\"] / eval_data_size\n",
    "    )\n",
    "    mean_rougeL_qj_aj[0][\"rouge-l\"][\"f\"] = (\n",
    "        mean_rougeL_qj_aj[0][\"rouge-l\"][\"f\"] / eval_data_size\n",
    "    )\n",
    "\n",
    "    return (eval_data_size, mean_rougeL_qj_c, mean_rougeL_aj_c, mean_rougeL_qj_aj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_RougeScores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0afab",
   "metadata": {},
   "source": [
    "## BLEU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf144a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu, bleu_score\n",
    "\n",
    "\n",
    "def get_average_BlueScores(data):\n",
    "    mean_bleu_aj_c = 0.0\n",
    "    mean_bleu_qj_c = 0.0\n",
    "    mean_bleu_qj_aj = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    bleu_aj_c = bleu(\n",
    "                        references=[context.split()],\n",
    "                        hypothesis=answers_joint.split(),\n",
    "                        weights=(1,),\n",
    "                    )\n",
    "                    bleu_qj_c = bleu(\n",
    "                        references=[context.split()],\n",
    "                        hypothesis=queries_joint.split(),\n",
    "                        weights=(1,),\n",
    "                    )\n",
    "                    bleu_qj_aj = bleu(\n",
    "                        references=[answers_joint.split()],\n",
    "                        hypothesis=queries_joint.split(),\n",
    "                        weights=(1,),\n",
    "                    )\n",
    "\n",
    "                    mean_bleu_aj_c = mean_bleu_aj_c + bleu_aj_c\n",
    "                    mean_bleu_qj_c = mean_bleu_qj_c + bleu_qj_c\n",
    "                    mean_bleu_qj_aj = mean_bleu_qj_aj + bleu_qj_aj\n",
    "\n",
    "    mean_bleu_aj_c = mean_bleu_aj_c / eval_data_size\n",
    "    mean_bleu_qj_c = mean_bleu_qj_c / eval_data_size\n",
    "    mean_bleu_qj_aj = mean_bleu_qj_aj / eval_data_size\n",
    "\n",
    "    return (eval_data_size, (mean_bleu_aj_c, mean_bleu_qj_c, mean_bleu_qj_aj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99166a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_BlueScores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11501069",
   "metadata": {},
   "source": [
    "## METEOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "def get_average_MeteorScores(data):\n",
    "\n",
    "    mean_meteor_q_a = 0.0\n",
    "    mean_meteor_q_c = 0.0\n",
    "    mean_meteor_a_c = 0.0\n",
    "    mean_meteor_aj_c = 0.0\n",
    "    mean_meteor_qj_c = 0.0\n",
    "    mean_meteor_qj_aj = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    meteor_aj_c = meteor(\n",
    "                        references=[word_tokenize(context)],\n",
    "                        hypothesis=word_tokenize(answers_joint),\n",
    "                    )\n",
    "                    meteor_qj_c = meteor(\n",
    "                        references=[word_tokenize(context)],\n",
    "                        hypothesis=word_tokenize(queries_joint),\n",
    "                    )\n",
    "                    meteor_qj_aj = meteor(\n",
    "                        references=[word_tokenize(answers_joint)],\n",
    "                        hypothesis=word_tokenize(queries_joint),\n",
    "                    )\n",
    "\n",
    "                    mean_meteor_aj_c = mean_meteor_aj_c + meteor_aj_c\n",
    "                    mean_meteor_qj_c = mean_meteor_qj_c + meteor_qj_c\n",
    "                    mean_meteor_qj_aj = mean_meteor_qj_aj + meteor_qj_aj\n",
    "\n",
    "                    mean_a_q = 0.0\n",
    "                    mean_c_q = 0.0\n",
    "                    mean_c_a = 0.0\n",
    "                    for q, a in zip(queries, answers):\n",
    "                        a_q = meteor(\n",
    "                            references=[word_tokenize(q)], hypothesis=word_tokenize(a)\n",
    "                        )\n",
    "                        c_q = meteor(\n",
    "                            references=[word_tokenize(context)],\n",
    "                            hypothesis=word_tokenize(q),\n",
    "                        )\n",
    "                        c_a = meteor(\n",
    "                            references=[word_tokenize(context)],\n",
    "                            hypothesis=word_tokenize(a),\n",
    "                        )\n",
    "                        mean_a_q = mean_a_q + a_q\n",
    "                        mean_c_q = mean_c_q + c_q\n",
    "                        mean_c_a = mean_c_a + c_a\n",
    "\n",
    "                    mean_a_q = mean_a_q / len(queries)\n",
    "                    mean_c_q = mean_c_q / len(queries)\n",
    "                    mean_c_a = mean_c_a / len(queries)\n",
    "\n",
    "                    mean_meteor_q_a = mean_meteor_q_a + mean_a_q\n",
    "                    mean_meteor_q_c = mean_meteor_q_c + mean_c_q\n",
    "                    mean_meteor_a_c = mean_meteor_a_c + mean_c_a\n",
    "\n",
    "    mean_meteor_q_a = mean_meteor_q_a / eval_data_size\n",
    "    mean_meteor_q_c = mean_meteor_q_c / eval_data_size\n",
    "    mean_meteor_a_c = mean_meteor_a_c / eval_data_size\n",
    "\n",
    "    mean_meteor_aj_c = mean_meteor_aj_c / eval_data_size\n",
    "    mean_meteor_qj_c = mean_meteor_qj_c / eval_data_size\n",
    "    mean_meteor_qj_aj = mean_meteor_qj_aj / eval_data_size\n",
    "\n",
    "    return (\n",
    "        eval_data_size,\n",
    "        (mean_meteor_a_c, mean_meteor_q_c, mean_meteor_q_a),\n",
    "        (mean_meteor_aj_c, mean_meteor_qj_c, mean_meteor_qj_aj),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd64a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_MeteorScores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d7167",
   "metadata": {},
   "source": [
    "## Jaccard Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(ref, hyp):\n",
    "    \"\"\"returns the jaccard similarity between two lists\"\"\"\n",
    "    intersection_cardinality = len(set.intersection(*[set(ref), set(hyp)]))\n",
    "    union_cardinality = len(set.union(*[set(ref), set(hyp)]))\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "\n",
    "def get_average_JaccardScores(data):\n",
    "    mean_aj_c = 0.0\n",
    "    mean_qj_c = 0.0\n",
    "    mean_qj_aj = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    aj_c = jaccard_similarity(context, answers_joint)\n",
    "                    qj_c = jaccard_similarity(context, queries_joint)\n",
    "                    qj_aj = jaccard_similarity(answers_joint, queries_joint)\n",
    "\n",
    "                    mean_aj_c = mean_aj_c + aj_c\n",
    "                    mean_qj_c = mean_qj_c + qj_c\n",
    "                    mean_qj_aj = mean_qj_aj + qj_aj\n",
    "\n",
    "    mean_aj_c = mean_aj_c / eval_data_size\n",
    "    mean_qj_c = mean_qj_c / eval_data_size\n",
    "    mean_qj_aj = mean_qj_aj / eval_data_size\n",
    "\n",
    "    return (eval_data_size, (mean_aj_c, mean_qj_c, mean_qj_aj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_JaccardScores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7653b",
   "metadata": {},
   "source": [
    "## Distinct-n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ba661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimw.app.services.eval.distinct_n import (\n",
    "    distinct_n_sentence_level,\n",
    "    distinct_n_corpus_level,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "def jaccard_similarity(ref, hyp):\n",
    "\n",
    "    \"\"\"returns the jaccard similarity between two lists\"\"\"\n",
    "\n",
    "    intersection_cardinality = len(set.intersection(*[set(ref), set(hyp)]))\n",
    "\n",
    "    union_cardinality = len(set.union(*[set(ref), set(hyp)]))\n",
    "\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_average_DistinctNScores(data):\n",
    "\n",
    "    mean_a_dist1 = 0.0\n",
    "    mean_q_dist1 = 0.0\n",
    "    mean_a_dist2 = 0.0\n",
    "    mean_q_dist2 = 0.0\n",
    "    mean_aj_dist1 = 0.0\n",
    "    mean_qj_dist1 = 0.0\n",
    "    mean_aj_dist2 = 0.0\n",
    "    mean_qj_dist2 = 0.0\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "\n",
    "                    # Extract targets\n",
    "\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "\n",
    "                    context = doc[\"doc\"]\n",
    "\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    a_dist1 = distinct_n_corpus_level(answers, 1)\n",
    "                    q_dist1 = distinct_n_corpus_level(queries, 1)\n",
    "\n",
    "                    a_dist2 = distinct_n_corpus_level(answers, 2)\n",
    "                    q_dist2 = distinct_n_corpus_level(queries, 2)\n",
    "\n",
    "                    aj_dist1 = distinct_n_sentence_level(answers_joint, 1)\n",
    "                    qj_dist1 = distinct_n_sentence_level(queries_joint, 1)\n",
    "\n",
    "                    aj_dist2 = distinct_n_sentence_level(answers_joint, 2)\n",
    "                    qj_dist2 = distinct_n_sentence_level(queries_joint, 2)\n",
    "\n",
    "                    mean_a_dist1 = mean_a_dist1 + a_dist1\n",
    "                    mean_q_dist1 = mean_q_dist1 + q_dist1\n",
    "                    mean_a_dist2 = mean_a_dist2 + a_dist2\n",
    "                    mean_q_dist2 = mean_q_dist2 + q_dist2\n",
    "                    mean_aj_dist1 = mean_aj_dist1 + aj_dist1\n",
    "                    mean_qj_dist1 = mean_qj_dist1 + qj_dist1\n",
    "                    mean_aj_dist2 = mean_aj_dist2 + aj_dist2\n",
    "                    mean_qj_dist2 = mean_qj_dist2 + qj_dist2\n",
    "\n",
    "    mean_a_dist1 = mean_a_dist1 / eval_data_size\n",
    "    mean_q_dist1 = mean_q_dist1 / eval_data_size\n",
    "    mean_a_dist2 = mean_a_dist2 / eval_data_size\n",
    "    mean_q_dist2 = mean_q_dist2 / eval_data_size\n",
    "    mean_aj_dist1 = mean_aj_dist1 / eval_data_size\n",
    "    mean_qj_dist1 = mean_qj_dist1 / eval_data_size\n",
    "    mean_aj_dist2 = mean_aj_dist2 / eval_data_size\n",
    "    mean_qj_dist2 = mean_qj_dist2 / eval_data_size\n",
    "\n",
    "    return (\n",
    "        eval_data_size,\n",
    "        (\n",
    "            mean_a_dist1,\n",
    "            mean_q_dist1,\n",
    "            mean_a_dist2,\n",
    "            mean_q_dist2,\n",
    "            mean_aj_dist1,\n",
    "            mean_qj_dist1,\n",
    "            mean_aj_dist2,\n",
    "            mean_qj_dist2,\n",
    "        ),\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba086b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_DistinctNScores = get_average_DistinctNScores(data)\n",
    "average_DistinctNScores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac5fe1",
   "metadata": {},
   "source": [
    "## G-Eval / DeepEval - Hallucinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from aimw.app.core.ai_config import get_ai_settings\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    get_ai_settings().openai_api_key\n",
    ")\n",
    "\n",
    "os.environ[\"DEEPEVAL_RESULTS_FOLDER\"] = \"./output/deepeval/hallucination\"\n",
    "\n",
    "# DeepEval Available GPT models: gpt-4o, gpt-4-turbo, gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4, gpt-4-32k, gpt-4-0613, gpt-4-32k-0613, gpt-3.5-turbo-1106, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125\n",
    "model_name = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "def get_average_HallucinationScores(data):\n",
    "    mean_score = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    test_case = LLMTestCase(\n",
    "                        input=queries_joint,\n",
    "                        actual_output=answers_joint,\n",
    "                        context=[context],\n",
    "                    )\n",
    "                    metric = HallucinationMetric(threshold=0.5, model=model_name)\n",
    "\n",
    "                    metric.measure(test_case)\n",
    "\n",
    "                    logger.info(metric.score)\n",
    "                    logger.info(metric.reason)\n",
    "                    logger.info\n",
    "\n",
    "                    mean_score = mean_score + metric.score\n",
    "                    # or evaluate test cases in bulk\n",
    "                    # evaluate([test_case], [metric])\n",
    "\n",
    "    mean_score = mean_score / eval_data_size\n",
    "\n",
    "    return (eval_data_size, mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_HallucinationScores(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ae07e",
   "metadata": {},
   "source": [
    "## DeepEval Contextual Relevancy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "def get_average_ContextualRelevancyScores(data):\n",
    "    mean_score = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    metric = ContextualRelevancyMetric(\n",
    "                        threshold=0.7, model=model_name, include_reason=True\n",
    "                    )\n",
    "                    test_case = LLMTestCase(\n",
    "                        input=queries_joint,\n",
    "                        actual_output=answers_joint,\n",
    "                        retrieval_context=[context],\n",
    "                    )\n",
    "\n",
    "                    metric.measure(test_case)\n",
    "                    logger.info(metric.score)\n",
    "                    logger.info(metric.reason)\n",
    "\n",
    "                    mean_score = mean_score + metric.score\n",
    "                    # or evaluate test cases in bulk\n",
    "                    # evaluate([test_case], [metric])\n",
    "\n",
    "    mean_score = mean_score / eval_data_size\n",
    "\n",
    "    return (eval_data_size, mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83029114",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualRelevancyScores = get_average_ContextualRelevancyScores(data)\n",
    "contextualRelevancyScores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab113120",
   "metadata": {},
   "source": [
    "## DeepEval Faithfulness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    HallucinationMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "def get_average_FaithfulnessMetric(data):\n",
    "    mean_score = 0.0\n",
    "\n",
    "    eval_data_size = 0\n",
    "\n",
    "    for i, doc in enumerate(data):  # data:\n",
    "        logger.info(f\"docid: {doc['docid']}\")\n",
    "        # if doc[\"docid\"] == 313017:\n",
    "        #     logger.info(doc)\n",
    "        if \"cct_saar\" in doc.keys():\n",
    "            if \"queries_aspects\" in doc[\"cct_saar\"].keys():\n",
    "                if len(doc[\"cct_saar\"][\"queries_aspects\"]) > 0:\n",
    "                    # Extract targets\n",
    "                    eval_data_size = eval_data_size + 1\n",
    "                    context = doc[\"doc\"]\n",
    "                    queries = [\n",
    "                        q[\"question\"] for q in doc[\"cct_saar\"][\"queries_aspects\"]\n",
    "                    ]\n",
    "                    answers = [a[\"answer\"] for a in doc[\"cct_saar\"][\"queries_aspects\"]]\n",
    "                    queries_joint = \" \".join(queries)\n",
    "                    answers_joint = \" \".join(answers)\n",
    "\n",
    "                    metric = FaithfulnessMetric(\n",
    "                        threshold=0.7, model=model_name, include_reason=True\n",
    "                    )\n",
    "                    test_case = LLMTestCase(\n",
    "                        input=queries_joint,\n",
    "                        actual_output=answers_joint,\n",
    "                        retrieval_context=[context],\n",
    "                    )\n",
    "\n",
    "                    metric.measure(test_case)\n",
    "                    logger.info(metric.score)\n",
    "                    logger.info(metric.reason)\n",
    "\n",
    "                    mean_score = mean_score + metric.score\n",
    "                    # or evaluate test cases in bulk\n",
    "                    # evaluate([test_case], [metric])\n",
    "\n",
    "    mean_score = mean_score / eval_data_size\n",
    "\n",
    "    return (eval_data_size, mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb51d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulnessMetric = get_average_FaithfulnessMetric(data)\n",
    "faithfulnessMetric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
