# command: --config /root/vllm-server-config.yaml # not working for the moment
services:
  vllm:
    image: ${VLLM_IMAGE:-vllm/vllm-openai:v0.9.0.1}
    container_name: ${CONTAINER_NAME:-vllm}
    restart: unless-stopped
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
      - ${TOOLS_CONF_PATH:-~/conf/tools}:/root/conf/tools
      - ${SSL_CONF_PATH:-~/conf/ssl}:/root/conf/ssl
      - ${CONFIG_FILE_PATH:-~/vllm/vllm-conf/vllm-server-config.yaml}:/root/vllm-server-config.yaml
    ports:
      - "${PORT:-7654}:${DOCKER_PORT:-8000}"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ipc: ${IPC_MODE:-host}
    command: >
      --model ${SERVED_MODEL_NAME}
      --hf-token ${HF_TOKEN}
      --tensor-parallel-size ${TENSOR_PARALLEL_SIZE}
      --enable-auto-tool-choice
      --tool-call-parser ${TOOL_CALL_PARSER}
      --ssl-keyfile ${SSL_KEYFILE}
      --ssl-certfile ${SSL_CERTFILE}
      --api-key ${API_KEY}
      --root-path ${ROOT_PATH}
      --dtype ${DTYPE}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION}
      --swap-space ${SWAP_SPACE}
      --disable-custom-all-reduce
      --cuda-graph-sizes ${CUDA_GRAPH_SIZES}
      --uvicorn-log-level ${UVICORN_LOG_LEVEL} 
      --config-format ${CONFIG_FORMAT}
      --cpu-offload-gb ${CPU_OFFLOAD_GB}
      --kv-cache-dtype ${KV_CACHE_DTYPE}
      --prefix-caching-hash-algo ${PREFIX_CACHING_HASH_ALGO}
