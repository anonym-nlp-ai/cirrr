# Docker container settings
CONTAINER_NAME=vllm
PORT=7654
DOCKER_PORT=8000
IPC_MODE=host
VLLM_IMAGE=vllm/vllm-openai:v0.9.0.1

# Volume mounts
HF_CACHE_PATH=~/.cache/huggingface
TOOLS_CONF_PATH=~/conf/tools
SSL_CONF_PATH=~/conf/ssl
CONFIG_FILE_PATH=~/vllm/vllm-conf/vllm-server-config.yaml

# Environment variables
HUGGING_FACE_HUB_TOKEN=hf_YOUR_HF_TOKEN_FOR_GATED_MODELS

# vLLM Server Configuration
API_KEY=4bcb5255-ecf0-49ff-967f-be4804fefcad
ROOT_PATH=https://127.0.0.1:7654
SSL_CERTFILE=/root/conf/ssl/server.crt
SSL_KEYFILE=/root/conf/ssl/server.key
TOOL_CALL_PARSER=pythonic
UVICORN_LOG_LEVEL=info

# Model Configuration
MODEL_NAME=google/gemma-3-27b-it
DTYPE=auto
SERVED_MODEL_NAME=${MODEL_NAME}
CONFIG_FORMAT=auto

# Parallel Configuration
TENSOR_PARALLEL_SIZE=4

# Cache Configuration
CPU_OFFLOAD_GB=0
GPU_MEMORY_UTILIZATION=0.9
KV_CACHE_DTYPE=auto
PREFIX_CACHING_HASH_ALGO=builtin
SWAP_SPACE=4

# Scheduler Configuration
CUDA_GRAPH_SIZES=512